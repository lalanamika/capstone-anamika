{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# custom tokenizer (moved to a separate module due to Streamlit requirements)\n",
    "import cust_tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14526, 7)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../data/interim/full_recipes_cleaned_2.csv')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>calories</th>\n",
       "      <th>rating</th>\n",
       "      <th>title</th>\n",
       "      <th>directionsStr</th>\n",
       "      <th>categoriesStr</th>\n",
       "      <th>ingredientsStr</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recipeId</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>426.0</td>\n",
       "      <td>2.500</td>\n",
       "      <td>Lentil, Apple, and Turkey Wrap</td>\n",
       "      <td>['1. Place the stock, lentils, celery, carrot,...</td>\n",
       "      <td>['Sandwich', 'Bean', 'Fruit', 'Tomato', 'turke...</td>\n",
       "      <td>['4 cups low-sodium vegetable or chicken stock...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>403.0</td>\n",
       "      <td>4.375</td>\n",
       "      <td>Boudin Blanc Terrine with Red Onion Confit</td>\n",
       "      <td>['Combine first 9 ingredients in heavy medium ...</td>\n",
       "      <td>['Food Processor', 'Onion', 'Pork', 'Bake', 'B...</td>\n",
       "      <td>['1 1/2 cups whipping cream', '2 medium onions...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>165.0</td>\n",
       "      <td>3.750</td>\n",
       "      <td>Potato and Fennel Soup Hodge</td>\n",
       "      <td>['In a large heavy saucepan cook diced fennel ...</td>\n",
       "      <td>['Soup/Stew', 'Dairy', 'Potato', 'Vegetable', ...</td>\n",
       "      <td>['1 fennel bulb (sometimes called anise), stal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>547.0</td>\n",
       "      <td>3.125</td>\n",
       "      <td>Spinach Noodle Casserole</td>\n",
       "      <td>['Preheat oven to 350°F. Lightly grease 8x8x2-...</td>\n",
       "      <td>['Cheese', 'Dairy', 'Pasta', 'Vegetable', 'Sid...</td>\n",
       "      <td>['1 12-ounce package frozen spinach soufflé, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>948.0</td>\n",
       "      <td>4.375</td>\n",
       "      <td>The Best Blts</td>\n",
       "      <td>['Mix basil, mayonnaise and butter in processo...</td>\n",
       "      <td>['Sandwich', 'Food Processor', 'Tomato', 'Kid-...</td>\n",
       "      <td>['2 1/2 cups (lightly packed) fresh basil leav...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20125</th>\n",
       "      <td>28.0</td>\n",
       "      <td>3.125</td>\n",
       "      <td>Parmesan Puffs</td>\n",
       "      <td>['Beat whites in a bowl with an electric mixer...</td>\n",
       "      <td>['Mixer', 'Cheese', 'Egg', 'Fry', 'Cocktail Pa...</td>\n",
       "      <td>['2 large egg whites', '3 oz Parmigiano-Reggia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20126</th>\n",
       "      <td>671.0</td>\n",
       "      <td>4.375</td>\n",
       "      <td>Artichoke and Parmesan Risotto</td>\n",
       "      <td>['Bring broth to simmer in saucepan.Remove fro...</td>\n",
       "      <td>['Side', 'Kid-Friendly', 'High Fiber', 'Dinner...</td>\n",
       "      <td>['5 1/2 cups (or more) low-salt chicken broth'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20127</th>\n",
       "      <td>563.0</td>\n",
       "      <td>4.375</td>\n",
       "      <td>Turkey Cream Puff Pie</td>\n",
       "      <td>['Using a sharp knife, cut a shallow X in bott...</td>\n",
       "      <td>['Onion', 'Poultry', 'turkey', 'Vegetable', 'B...</td>\n",
       "      <td>['1 small tomato', '1 small onion, finely chop...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20128</th>\n",
       "      <td>631.0</td>\n",
       "      <td>4.375</td>\n",
       "      <td>Snapper on Angel Hair with Citrus Cream</td>\n",
       "      <td>['Heat 2 tablespoons oil in heavy medium skill...</td>\n",
       "      <td>['Milk/Cream', 'Citrus', 'Dairy', 'Fish', 'Gar...</td>\n",
       "      <td>['4 tablespoons olive oil', '4 shallots, thinl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20129</th>\n",
       "      <td>560.0</td>\n",
       "      <td>4.375</td>\n",
       "      <td>Baked Ham with Marmalade-Horseradish Glaze</td>\n",
       "      <td>['Position rack in bottom third of oven and pr...</td>\n",
       "      <td>['Pork', 'Bake', 'Roast', 'Christmas', 'Ham', ...</td>\n",
       "      <td>['1 18-pound fully cooked bone-in smoked ham, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14526 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          calories  rating                                       title  \\\n",
       "recipeId                                                                 \n",
       "0            426.0   2.500              Lentil, Apple, and Turkey Wrap   \n",
       "1            403.0   4.375  Boudin Blanc Terrine with Red Onion Confit   \n",
       "2            165.0   3.750                Potato and Fennel Soup Hodge   \n",
       "4            547.0   3.125                    Spinach Noodle Casserole   \n",
       "5            948.0   4.375                               The Best Blts   \n",
       "...            ...     ...                                         ...   \n",
       "20125         28.0   3.125                              Parmesan Puffs   \n",
       "20126        671.0   4.375              Artichoke and Parmesan Risotto   \n",
       "20127        563.0   4.375                       Turkey Cream Puff Pie   \n",
       "20128        631.0   4.375     Snapper on Angel Hair with Citrus Cream   \n",
       "20129        560.0   4.375  Baked Ham with Marmalade-Horseradish Glaze   \n",
       "\n",
       "                                              directionsStr  \\\n",
       "recipeId                                                      \n",
       "0         ['1. Place the stock, lentils, celery, carrot,...   \n",
       "1         ['Combine first 9 ingredients in heavy medium ...   \n",
       "2         ['In a large heavy saucepan cook diced fennel ...   \n",
       "4         ['Preheat oven to 350°F. Lightly grease 8x8x2-...   \n",
       "5         ['Mix basil, mayonnaise and butter in processo...   \n",
       "...                                                     ...   \n",
       "20125     ['Beat whites in a bowl with an electric mixer...   \n",
       "20126     ['Bring broth to simmer in saucepan.Remove fro...   \n",
       "20127     ['Using a sharp knife, cut a shallow X in bott...   \n",
       "20128     ['Heat 2 tablespoons oil in heavy medium skill...   \n",
       "20129     ['Position rack in bottom third of oven and pr...   \n",
       "\n",
       "                                              categoriesStr  \\\n",
       "recipeId                                                      \n",
       "0         ['Sandwich', 'Bean', 'Fruit', 'Tomato', 'turke...   \n",
       "1         ['Food Processor', 'Onion', 'Pork', 'Bake', 'B...   \n",
       "2         ['Soup/Stew', 'Dairy', 'Potato', 'Vegetable', ...   \n",
       "4         ['Cheese', 'Dairy', 'Pasta', 'Vegetable', 'Sid...   \n",
       "5         ['Sandwich', 'Food Processor', 'Tomato', 'Kid-...   \n",
       "...                                                     ...   \n",
       "20125     ['Mixer', 'Cheese', 'Egg', 'Fry', 'Cocktail Pa...   \n",
       "20126     ['Side', 'Kid-Friendly', 'High Fiber', 'Dinner...   \n",
       "20127     ['Onion', 'Poultry', 'turkey', 'Vegetable', 'B...   \n",
       "20128     ['Milk/Cream', 'Citrus', 'Dairy', 'Fish', 'Gar...   \n",
       "20129     ['Pork', 'Bake', 'Roast', 'Christmas', 'Ham', ...   \n",
       "\n",
       "                                             ingredientsStr  \n",
       "recipeId                                                     \n",
       "0         ['4 cups low-sodium vegetable or chicken stock...  \n",
       "1         ['1 1/2 cups whipping cream', '2 medium onions...  \n",
       "2         ['1 fennel bulb (sometimes called anise), stal...  \n",
       "4         ['1 12-ounce package frozen spinach soufflé, t...  \n",
       "5         ['2 1/2 cups (lightly packed) fresh basil leav...  \n",
       "...                                                     ...  \n",
       "20125     ['2 large egg whites', '3 oz Parmigiano-Reggia...  \n",
       "20126     ['5 1/2 cups (or more) low-salt chicken broth'...  \n",
       "20127     ['1 small tomato', '1 small onion, finely chop...  \n",
       "20128     ['4 tablespoons olive oil', '4 shallots, thinl...  \n",
       "20129     ['1 18-pound fully cooked bone-in smoked ham, ...  \n",
       "\n",
       "[14526 rows x 6 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.set_index('recipeId')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null values: 0\n",
      "Duplicated rows: 0\n"
     ]
    }
   ],
   "source": [
    "# confirm that there are no null values or duplicated values\n",
    "print(f\"Null values: {df.isna().sum().sum()}\")\n",
    "print(f\"Duplicated rows: {df.duplicated().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\anami\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(tokenizer=(cust_tokenizer.my_tokenizer),\n",
    "                       min_df=5)\n",
    "ingredients_matrix = vectorizer.fit_transform(df['ingredientsStr'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "{'lowsodium': 1152, 'vegetable': 2072, 'chicken': 333, 'stock': 1967, 'brown': 210, 'lentils': 1100, 'french': 760, 'green': 858, 'celery': 303, 'carrot': 285, 'sprig': 1947, 'thyme': 2028, 'kosher': 1055, 'salt': 1752, 'tomato': 2041, 'cored': 449, 'seeded': 1803, 'diced': 576, 'fuji': 776, 'apple': 37, 'freshly': 764, 'lemon': 1092, 'juice': 1012, 'extravirgin': 667, 'olive': 1361, 'oil': 1354, 'pepper': 1460, 'taste': 2013, 'sheets': 1844, 'wholewheat': 2116, 'lavash': 1073, 'half': 885, 'crosswise': 494, 'flour': 734, 'tortillas': 2046, 'turkey': 2061, 'breast': 193, 'head': 910, 'bibb': 109, 'lettuce': 1101, 'whipping': 2102, 'cream': 478, 'onions': 1365, 'bay': 81, 'leaves': 1081, 'cloves': 389, 'garlic': 788, 'clove': 387, 'nutmeg': 1345, 'shallots': 1829, 'butter': 235, 'boneless': 157, 'center': 305, 'pork': 1546, 'loin': 1139, 'sinew': 1877, 'chunks': 369, 'chilled': 345, 'eggs': 633, 'purpose': 1601, 'tawny': 2014, 'port': 1547, 'currants': 525, 'peppercorns': 1461, 'parsley': 1422, 'bread': 188, 'baguette': 63, 'slices': 1898, 'red': 1647, 'halved': 890, 'wine': 2120, 'vinegar': 2087, 'canned': 265, 'broth': 209, 'sugar': 1979, 'fennel': 683, 'bulb': 220, 'called': 255, 'anise': 31, 'dice': 575, 'reserved': 1670, 'garnish': 791, 'onion': 1364, 'russet': 1735, 'potatoes': 1558, 'milk': 1262, 'package': 1394, 'frozen': 770, 'spinach': 1946, 'souffl': 1924, 'egg': 630, 'noodles': 1336, 'cooked': 440, 'sour': 1926, 'purchased': 1596, 'pesto': 1473, 'sauce': 1764, 'sharp': 1838, 'cheddar': 322, 'cheese': 323, 'lightly': 1108, 'packed': 1397, 'basil': 78, 'mayonnaise': 1217, 'room': 1712, 'bacon': 62, 'strips': 1974, 'inchthick': 970, 'countrystyle': 467, 'tomatoes': 2042, 'rounds': 1728, 'ripe': 1694, 'avocados': 58, 'pitted': 1514, 'soy': 1929, 'asian': 55, 'sesame': 1823, 'bunch': 224, 'scallions': 1776, 'pale': 1403, 'parts': 1426, 'separated': 1816, 'greens': 859, 'ginger': 807, 'seeds': 1805, 'toasted': 2035, 'flank': 714, 'steak': 1956, 'grain': 834, 'packaged': 1395, 'kimchi': 1039, 'rice': 1678, 'long': 1141, 'sprigs': 1948, 'reducedsodium': 1653, 'gelatin': 794, 'envelope': 647, 'piece': 1489, 'ham': 892, 'cubes': 513, 'dijon': 577, 'mustard': 1305, 'whitewine': 2111, 'ribs': 1677, 'cornichons': 458, 'gherkins': 803, 'boiled': 150, 'box': 175, 'peas': 1445, 'marjoram': 1197, 'equipment': 652, 'wide': 2117, 'jars': 1002, 'containers': 436, 'rosemary': 1718, 'yams': 2142, 'redskinned': 1651, 'sweet': 1989, 'lowsalt': 1151, 'nonstick': 1335, 'allpurpose': 11, 'soda': 1913, 'sticks': 1962, 'light': 1105, 'vanilla': 2069, 'extract': 663, 'mashed': 1209, 'bananas': 70, 'mini': 1264, 'chocolate': 357, 'chips': 354, 'creamy': 481, 'peanut': 1440, 'powdered': 1563, 'cake': 248, 'pans': 1409, 'beef': 90, 'tenderloin': 2018, 'steaks': 1957, 'brandy': 185, 'peach': 1438, 'cider': 373, 'chives': 356, 'andor': 26, 'vegetables': 2074, 'escarole': 653, 'optional': 1369, 'california': 253, 'avocado': 57, 'cilantro': 374, 'shoyu': 1862, 'sea': 1789, 'preferably': 1567, 'shallot': 1828, 'scallion': 1775, 'dry': 605, 'lime': 1111, 'mint': 1267, 'jalapeo': 994, 'including': 972, 'portabella': 1548, 'mushroom': 1302, 'cap': 270, 'buttermilk': 240, 'heavy': 920, 'honey': 947, 'scant': 1779, 'coarse': 391, 'cornmeal': 460, 'grits': 870, 'tartar': 2011, 'strawberry': 1970, 'preserves': 1571, 'shoulder': 1861, 'lean': 1080, 'carrots': 286, 'pieces': 1490, 'turkish': 2062, 'juniper': 1018, 'berries': 102, 'coriander': 451, 'cumin': 517, 'pot': 1556, 'lid': 1103, 'bell': 96, 'peppers': 1465, 'lengthwise': 1099, 'left': 1087, 'intact': 986, 'anchovy': 25, 'fillets': 694, 'drained': 595, 'mozzarella': 1292, 'regular': 1659, 'champagne': 312, 'coarsely': 396, 'asparagus': 56, 'quartered': 1607, 'capers': 274, 'patted': 1435, 'mixed': 1275, 'spring': 1949, 'radishes': 1625, 'hardboiled': 900, 'imported': 967, 'tuna': 2058, 'chive': 355, 'blossoms': 139, 'slice': 1896, 'paste': 1433, 'seasoned': 1793, 'chile': 339, 'pear': 1442, 'watercress': 2098, 'bunches': 225, 'shredded': 1863, 'markets': 1198, 'mail': 1166, 'order': 1374, 'pecan': 1447, 'halves': 891, 'melted': 1241, 'fine': 697, 'crumbs': 501, 'hardcooked': 901, 'cucumbers': 515, 'cucumber': 514, 'cinnamon': 375, 'paperthin': 1413, 'boiling': 151, 'redwine': 1654, 'beans': 84, 'kalamata': 1024, 'nioise': 1328, 'olives': 1362, 'lowfat': 1150, 'orange': 1370, 'peel': 1451, 'powder': 1562, 'shortening': 1858, 'cornstarch': 461, 'apricot': 42, 'nectar': 1317, 'apricots': 43, 'bing': 111, 'cherries': 326, 'inchwide': 971, 'reserving': 1671, 'separately': 1817, 'sherry': 1849, 'dijonstyle': 578, 'stick': 1961, 'softened': 1916, 'pickle': 1483, 'yellow': 2144, 'skin': 1886, 'oilcured': 1355, 'rinsed': 1693, 'sage': 1745, 'broccoli': 206, 'rabe': 1620, 'grouper': 871, 'snapper': 1909, 'fillet': 693, 'skinned': 1888, 'corn': 454, 'dusting': 615, 'yeast': 2143, 'canola': 267, 'beaten': 87, 'ridged': 1685, 'grill': 864, 'pan': 1406, 'castiron': 297, 'skillet': 1884, 'griddle': 863, 'coconut': 403, 'virgin': 2089, 'key': 1035, 'zest': 2155, 'limes': 1113, 'yolks': 2148, 'cold': 414, 'flakes': 711, 'halibut': 888, 'orzo': 1381, 'riceshaped': 1680, 'pasta': 1432, 'cherry': 327, 'oregano': 1376, 'habanero': 881, 'hot': 952, 'latin': 1071, 'cover': 469, 'overnight': 1388, 'quicksoaked': 1613, 'procedure': 1577, 'follows': 744, 'hock': 937, 'leaf': 1078, 'organo': 1378, 'chili': 341, 'italian': 991, 'radish': 1624, 'skinless': 1887, 'tortilla': 2045, 'chilies': 342, 'grapefruits': 844, 'pink': 1504, 'navel': 1312, 'oranges': 1373, 'pomegranate': 1540, 'cranberries': 476, 'inchlong': 969, 'ancho': 23, 'mexican': 1250, 'mushrooms': 1303, 'latino': 1072, 'foods': 747, 'shops': 1855, 'wheat': 2100, 'germ': 800, 'chunky': 370, 'dark': 537, 'hearts': 915, 'livers': 1132, 'chickens': 335, 'crustless': 503, 'country': 466, 'walnuts': 2096, 'blend': 133, 'parmesan': 1419, 'pancetta': 1407, 'bulbs': 221, 'selfrising': 1809, 'whites': 2109, 'superfine': 1986, 'almond': 13, 'natural': 1311, 'syrup': 1994, 'veal': 2071, 'worcestershire': 2131, 'coarsegrained': 393, 'rubbed': 1730, 'inchdiameter': 968, 'portobello': 1553, 'sesameseed': 1824, 'buns': 227, 'golden': 826, 'raisins': 1628, 'allspice': 12, 'cooled': 447, 'pure': 1597, 'pumpkin': 1594, 'raw': 1638, 'hulled': 956, 'pepitas': 1458, 'muffin': 1294, 'concentrate': 429, 'balsamic': 67, 'eggplants': 632, 'pita': 1511, 'breads': 191, 'horizontally': 950, 'brinecured': 198, 'saga': 1744, 'blue': 141, 'chipotle': 352, 'chiles': 340, 'adobo': 2, 'ears': 621, 'shucked': 1865, 'firmripe': 705, 'romaine': 1709, 'outer': 1383, 'anaheim': 22, 'poblano': 1532, 'packages': 1396, 'petite': 1476, 'kernels': 1033, 'day': 545, 'old': 1359, 'lard': 1065, 'little': 1127, 'bourbon': 170, 'yolk': 2147, 'sorghum': 1922, 'granulated': 840, 'strawberries': 1969, 'blackberries': 123, 'electric': 638, 'mixer': 1276, 'extrafirm': 664, 'tofu': 2038, 'pressed': 1573, 'granules': 841, 'flaxseed': 728, 'sunflower': 1984, 'hazelnuts': 909, 'husked': 959, 'ice': 961, 'bittersweet': 122, 'semisweet': 1813, 'goodquality': 828, 'lindt': 1114, 'bakers': 65, 'grand': 837, 'marnier': 1200, 'liqueur': 1124, 'aleppo': 9, 'bulgur': 222, 'butternut': 241, 'squash': 1952, 'noboil': 1329, 'lasagne': 1070, 'stilton': 1964, 'prepared': 1569, 'horseradish': 951, 'xinch': 2139, 'sourdough': 1927, 'roast': 1696, 'arugula': 53, 'raspberries': 1635, 'shelled': 1846, 'nuts': 1347, 'container': 435, 'ricotta': 1684, 'set': 1825, 'cayenne': 301, 'jcama': 1004, 'produce': 1580, 'rind': 1690, 'pith': 1512, 'free': 756, 'serrated': 1820, 'knife': 1049, 'membranes': 1243, 'acorn': 1, 'chestnuts': 330, 'maple': 1190, 'custard': 527, 'ramekins': 1629, 'basmati': 79, 'thighs': 2027, 'ones': 1363, 'organic': 1377, 'slivered': 1901, 'blanched': 131, 'almonds': 14, 'pistachio': 1509, 'saffron': 1743, 'rose': 1717, 'cardamom': 281, 'pods': 1535, 'handful': 895, 'fava': 679, 'plain': 1518, 'yogurt': 2146, 'bottled': 165, 'dill': 579, 'bones': 158, 'tomatillos': 2040, 'serrano': 1819, 'pistachios': 1510, 'matchsticksize': 1213, 'turmeric': 2063, 'ingredient': 977, 'info': 976, 'sold': 1918, 'middle': 1257, 'eastern': 625, 'kalustyanscom': 1026, 'paprika': 1416, 'flatcut': 720, 'brisket': 204, 'layer': 1075, 'fat': 676, 'grapeseed': 846, 'bottle': 164, 'stout': 1968, 'plum': 1528, 'jam': 996, 'chops': 364, 'ketchup': 1034, 'tabasco': 1996, 'bonein': 156, 'legs': 1090, 'thigh': 2026, 'drumstick': 603, 'hazelnut': 908, 'walnut': 2095, 'mango': 1183, 'salmon': 1750, 'mediumsize': 1234, 'cans': 268, 'pinto': 1506, 'lamb': 1063, 'shanks': 1833, 'redpepper': 1650, 'cabbage': 244, 'root': 1714, 'celeriac': 302, 'roquefort': 1716, 'savoy': 1772, 'beets': 94, 'fronds': 768, 'clam': 379, 'seasoning': 1794, 'shrimp': 1864, 'potato': 1557, 'parsnips': 1424, 'leek': 1085, 'deepfrying': 555, 'parsnip': 1423, 'blackberry': 124, 'bottles': 166, 'oatmeal': 1349, 'beer': 92, 'guinness': 877, 'fullfat': 778, 'roughly': 1723, 'serving': 1822, 'crusty': 505, 'peasant': 1446, 'round': 1725, 'loaf': 1133, 'ricestick': 1681, 'vermicelli': 2081, 'matchsticks': 1212, 'chinese': 349, 'lengths': 1098, 'confit': 434, 'duck': 608, 'herbs': 928, 'hoisin': 939, 'sriracha': 1954, 'southeast': 1928, 'chinkiang': 351, 'sweetened': 1990, 'coco': 401, 'lpez': 1154, 'rum': 1734, 'curry': 526, 'pineapple': 1502, 'flaked': 710, 'mild': 1260, 'matzoh': 1215, 'pearl': 1443, 'quinces': 1617, 'gold': 825, 'cocktail': 400, 'guava': 876, 'grater': 849, 'food': 746, 'processor': 1579, 'holes': 941, 'salsa': 1751, 'verde': 2079, 'chickpeas': 337, 'feta': 686, 'brioche': 200, 'pumpernickel': 1593, 'pickedover': 1482, 'dash': 540, 'drops': 602, 'angostura': 30, 'bitters': 121, 'club': 390, 'layers': 1076, 'centercut': 306, 'cod': 404, 'swiss': 1991, 'chard': 317, 'roasted': 1697, 'amaretto': 18, 'angel': 28, 'hair': 883, 'peanuts': 1441, 'thai': 2025, 'section': 1798, 'soft': 1915, 'goat': 821, 'montrachet': 1285, 'yukon': 2153, 'baguettes': 64, 'sausages': 1767, 'homemade': 944, 'apples': 39, 'picked': 1481, 'branches': 182, 'burrata': 230, 'bocconcini': 147, 'watermelon': 2099, 'greasing': 854, 'winter': 2123, 'blueberries': 142, 'whipped': 2101, 'wings': 2122, 'rib': 1674, 'kombu': 1053, 'seaweed': 1796, 'japanese': 998, 'cool': 446, 'shaved': 1839, 'katsuobushi': 1027, 'bonito': 159, 'shiitake': 1850, 'dashi': 542, 'recipe': 1642, 'mirin': 1271, 'korean': 1054, 'deveined': 570, 'englishstyle': 646, 'gingerroot': 808, 'desired': 567, 'julienne': 1015, 'pockets': 1533, 'brewed': 195, 'jasmine': 1003, 'tea': 2015, 'roses': 1719, 'vermouth': 2082, 'cointreau': 410, 'orangeflavored': 1371, 'raspberry': 1636, 'oriental': 1379, 'diagonal': 571, 'bok': 153, 'choy': 366, 'neck': 1315, 'heart': 914, 'gizzard': 811, 'andouille': 27, 'sausage': 1766, 'breadcrumbs': 190, 'firm': 703, 'sandwich': 1761, 'wafer': 2091, 'cookies': 443, 'broken': 208, 'slightly': 1899, 'miniature': 1265, 'extrasharp': 666, 'pickled': 1484, 'jar': 1000, 'liquid': 1125, 'oilpacked': 1357, 'sundried': 1983, 'grapefruit': 843, 'ruby': 1733, 'segments': 1807, 'rings': 1692, 'cranberry': 477, 'pears': 1444, 'bowl': 172, 'firmly': 704, 'bits': 119, 'molasses': 1280, 'crust': 502, 'caps': 275, 'cutlets': 528, 'pounded': 1560, 'lemongrass': 1094, 'lemons': 1096, 'diameter': 573, 'crackers': 475, 'tenders': 2020, 'graham': 833, 'cracker': 474, 'blueberry': 143, 'tart': 2010, 'scotch': 1784, 'whisky': 2106, 'delicious': 561, 'glaze': 814, 'yellowfleshed': 2145, 'skirt': 1891, 'crema': 482, 'jumbo': 1017, 'lump': 1156, 'crabmeat': 472, 'cornflakes': 457, 'peeler': 1452, 'hearty': 916, 'cabernet': 245, 'sauvignon': 1769, 'zinfandel': 2157, 'syrah': 1993, 'chuck': 367, 'roasts': 1699, 'heaping': 913, 'turnips': 2065, 'brussels': 215, 'sprouts': 1951, 'pine': 1501, 'scrubbed': 1788, 'leaving': 1082, 'flatleaf': 721, 'citrus': 378, 'navy': 1313, 'cannellini': 266, 'slab': 1893, 'kabocha': 1020, 'marcona': 1192, 'seed': 1802, 'tied': 2030, 'cooks': 445, 'note': 1340, 'sheet': 1843, 'puff': 1589, 'pastry': 1434, 'banana': 69, 'finequality': 699, 'tarragon': 2009, 'malt': 1177, 'serve': 1821, 'breasts': 194, 'prunes': 1586, 'gravy': 852, 'grenadine': 860, 'cognac': 408, 'quarter': 1606, 'heated': 918, 'spanish': 1934, 'bonnet': 160, 'annatto': 35, 'bean': 83, 'plantain': 1519, 'calabrian': 251, 'applewoodsmoked': 41, 'fully': 779, 'linguia': 1119, 'forest': 750, 'longgrain': 1142, 'neutral': 1323, 'casing': 292, 'coins': 409, 'honeycrisp': 948, 'gala': 783, 'scallops': 1778, 'dredging': 596, 'flatleafed': 722, 'currant': 523, 'grape': 842, 'heirloom': 922, 'colors': 421, 'frying': 775, 'fish': 707, 'filet': 691, 'equal': 651, 'zucchini': 2159, 'cocoa': 402, 'prune': 1585, 'margarine': 1193, 'granny': 838, 'smith': 1906, 'kale': 1025, 'instant': 981, 'coffee': 405, 'phyllo': 1479, 'paper': 1412, 'covered': 470, 'dampened': 533, 'kitchen': 1044, 'cola': 411, 'cutup': 531, 'bowls': 173, 'kiwi': 1045, 'spiced': 1943, 'spice': 1942, 'epirecipelink': 650, 'sparkling': 1936, 'measuring': 1224, 'vodka': 2090, 'gin': 806, 'tequila': 2023, 'diagonally': 572, 'parmigianoreggiano': 1421, 'shavings': 1840, 'monterey': 1284, 'jack': 993, 'size': 1882, 'sirloin': 1879, 'fontina': 745, 'gruyre': 872, 'cauliflower': 299, 'core': 448, 'end': 641, 'wasabi': 2097, 'bamboo': 68, 'skewers': 1883, 'minutes': 1270, 'dutchprocess': 617, 'extralarge': 665, 'espresso': 654, 'confectioners': 433, 'guajillo': 874, 'nigella': 1326, 'indian': 973, 'online': 1366, 'mortar': 1289, 'pestle': 1472, 'rye': 1741, 'aged': 5, 'parchment': 1417, 'frise': 767, 'curly': 522, 'chicory': 338, 'radicchio': 1623, 'hocks': 938, 'needed': 1321, 'giblets': 805, 'hour': 954, 'chiligarlic': 343, 'heads': 912, 'dough': 592, 'wild': 2118, 'crimini': 486, 'oyster': 1390, 'gouda': 831, 'pizza': 1515, 'boboli': 146, 'cubed': 512, 'dressing': 597, 'pepperoni': 1464, 'inside': 980, 'necessary': 1314, 'ends': 644, 'porcini': 1545, 'button': 243, 'cremini': 484, 'polenta': 1538, 'chickpea': 336, 'loosely': 1145, 'roasting': 1698, 'rack': 1621, 'pickling': 1486, 'spices': 1944, 'tips': 2033, 'packet': 1398, 'udon': 2067, 'snipped': 1910, 'scalded': 1773, 'meal': 1221, 'good': 827, 'dates': 544, 'medjool': 1236, 'manischewitz': 1188, 'extra': 662, 'malaga': 1174, 'date': 543, 'dyed': 618, 'pecans': 1448, 'cerignola': 309, 'quarters': 1608, 'variety': 2070, 'brands': 184, 'quinoa': 1618, 'xanthan': 2138, 'gum': 878, 'nondairy': 1331, 'condensed': 432, 'pie': 1488, 'casings': 293, 'linguine': 1120, 'nonfat': 1332, 'oats': 1350, 'catfish': 298, 'quickcooking': 1611, 'florets': 732, 'mix': 1274, 'sole': 1919, 'haddock': 882, 'rock': 1702, 'lobster': 1135, 'tail': 2000, 'shell': 1845, 'scissors': 1781, 'meat': 1225, 'king': 1041, 'crab': 471, 'leg': 1089, 'garbanzo': 787, 'meaty': 1226, 'short': 1856, 'cacao': 246, 'ribeye': 1676, 'comice': 424, 'relish': 1660, 'breadandbutter': 189, 'pickles': 1485, 'cellophane': 304, 'known': 1051, 'jalapeos': 995, 'tails': 2001, 'kumquats': 1056, 'tender': 2017, 'excess': 660, 'caraway': 278, 'gorgonzola': 830, 'dolce': 586, 'boned': 155, 'butterflied': 238, 'butcher': 232, 'parmigiano': 1420, 'reggiano': 1658, 'crookneck': 492, 'cantaloupe': 269, 'crystallized': 507, 'leeks': 1086, 'wholemilk': 2115, 'grilling': 866, 'charcoal': 316, 'chimney': 347, 'xxinch': 2140, 'seedless': 1804, 'grapes': 845, 'flounder': 733, 'roughy': 1724, 'puree': 1599, 'artichokes': 52, 'drumsticks': 604, 'morel': 1287, 'crme': 490, 'frache': 753, 'stripped': 1973, 'rbol': 1639, 'instantread': 984, 'maker': 1171, 'rubbing': 1732, 'butt': 234, 'tangerine': 2004, 'fivespice': 709, 'bone': 154, 'frenched': 762, 'mizuna': 1278, 'kirsch': 1043, 'cookie': 442, 'cutter': 529, 'gingersnap': 809, 'gingersnaps': 810, 'exceed': 659, 'fluted': 739, 'deep': 551, 'removable': 1663, 'triple': 2050, 'sec': 1797, 'shortgrain': 1859, 'sushi': 1987, 'amaretti': 17, 'macaroons': 1160, 'blackcurrant': 125, 'icecream': 964, 'candy': 262, 'chutney': 371, 'mandoline': 1182, 'fitted': 708, 'blade': 129, 'deepfat': 553, 'lukewarm': 1155, 'brushing': 214, 'chorizo': 365, 'silken': 1872, 'italianstyle': 992, 'juices': 1014, 'mussels': 1304, 'debearded': 547, 'dashes': 541, 'fettuccine': 687, 'mascarpone': 1208, 'dayold': 546, 'forced': 749, 'press': 1572, 'cultivated': 516, 'slicer': 1897, 'pinches': 1500, 'cooking': 444, 'bratwurst': 187, 'sauerkraut': 1765, 'kielbasa': 1037, 'alsatian': 15, 'pinot': 1505, 'blanc': 130, 'greek': 856, 'persian': 1469, 'deepfry': 554, 'refrigerated': 1656, 'anjou': 34, 'easy': 626, 'knuckle': 1052, 'coffeespice': 407, 'grinder': 868, 'english': 645, 'philadelphiabrand': 1478, 'sieve': 1869, 'agave': 4, 'ribbons': 1675, 'nectarines': 1319, 'darkbrown': 538, 'generous': 797, 'spaghetti': 1932, 'slivers': 1902, 'swordfish': 1992, 'portions': 1552, 'benriner': 101, 'shallow': 1830, 'jicama': 1007, 'spicy': 1945, 'american': 21, 'mediumdry': 1229, 'sides': 1868, 'new': 1324, 'boxes': 176, 'mediumgrain': 1230, 'buffalo': 219, 'calvados': 256, 'demerara': 562, 'mangoes': 1184, 'zested': 2156, 'juiced': 1013, 'haricots': 903, 'verts': 2083, 'prosciutto': 1581, 'garam': 786, 'masala': 1207, 'ghee': 801, 'clarified': 381, 'sifted': 1871, 'notes': 1341, 'starch': 1955, 'beating': 88, 'plums': 1530, 'fruit': 771, 'cashews': 291, 'foil': 742, 'kiwifruit': 1046, 'mexico': 1253, 'wiped': 2124, 'clean': 382, 'hominy': 946, 'queso': 1609, 'fresco': 763, 'iceberg': 962, 'fried': 766, 'belgian': 95, 'endive': 642, 'essencia': 655, 'muscat': 1298, 'dessert': 569, 'deglet': 557, 'papery': 1414, 'husks': 960, 'remove': 1664, 'sticky': 1963, 'coating': 397, 'frangelico': 755, 'herbes': 927, 'provence': 1583, 'noor': 1337, 'barbecue': 72, 'kaiser': 1023, 'rolls': 1707, 'coleslaw': 415, 'slit': 1900, 'greekstyle': 857, 'mung': 1297, 'glutinous': 819, 'vietnamese': 2085, 'nuoc': 1342, 'mam': 1179, 'soybean': 1930, 'mediumlarge': 1233, 'fruits': 772, 'log': 1137, 'eighteen': 634, 'maui': 1216, 'hours': 955, 'dont': 587, 'dryroasted': 606, 'capellini': 272, 'bran': 181, 'doubleacting': 589, 'truffle': 2054, 'chanterelle': 313, 'bitesize': 117, 'honeydew': 949, 'melon': 1238, 'papaya': 1410, 'combination': 422, 'lining': 1121, 'rutabaga': 1739, 'mixture': 1277, 'just': 1019, 'flaky': 712, 'individual': 974, 'pernod': 1468, 'aniseflavored': 33, 'picholine': 1480, 'concentrated': 430, 'peppermint': 1463, 'grating': 851, 'edamame': 628, 'soybeans': 1931, 'pecorino': 1449, 'romano': 1710, 'salted': 1753, 'links': 1123, 'breakfast': 192, 'sections': 1800, 'endives': 643, 'inner': 979, 'crisp': 487, 'marinated': 1196, 'artichoke': 51, 'peaches': 1439, 'tamarind': 2003, 'napa': 1309, 'gyoza': 880, 'wrappers': 2135, 'stewed': 1959, 'wonton': 2127, 'marmalade': 1199, 'tritip': 2051, 'headon': 911, 'shellon': 1847, 'pured': 1598, 'squid': 1953, 'boston': 163, 'look': 1143, 'maraschino': 1191, 'fig': 689, 'amber': 20, 'grade': 832, 'clear': 384, 'salata': 1749, 'cube': 511, 'flat': 716, 'cheesecloth': 324, 'european': 656, 'freezedried': 758, 'cleaned': 383, 'scaled': 1774, 'whitefish': 2107, 'soup': 1925, 'sorrel': 1923, 'seasonally': 1792, 'herb': 925, 'edible': 629, 'flowers': 737, 'nonreactive': 1334, 'heavyduty': 921, 'tahini': 1999, 'hothouse': 953, 'halfmoons': 887, 'oysters': 1391, 'point': 1536, 'pacific': 1392, 'liquor': 1126, 'caper': 273, 'gnocchi': 820, 'charred': 319, 'flesh': 730, 'sectioned': 1799, 'portion': 1551, 'palegreen': 1404, 'sprouted': 1950, 'larger': 1068, 'burner': 229, 'poultry': 1559, 'gas': 793, 'oven': 1386, 'case': 289, 'mahimahi': 1165, 'wooden': 2129, 'eggplant': 631, 'decorative': 550, 'cutters': 530, 'epazote': 649, 'cured': 520, 'major': 1169, 'greys': 862, 'homemadetype': 945, 'plasticwrapped': 1522, 'scrod': 1787, 'gray': 853, 'roe': 1703, 'mold': 1281, 'scraper': 1786, 'offset': 1353, 'spatula': 1937, 'salad': 1747, 'lopez': 1147, 'evaporated': 658, 'labeled': 1057, 'verbena': 2078, 'sambuca': 1758, 'ciabatta': 372, 'brie': 196, 'sanding': 1760, 'grainy': 835, 'arrowroot': 50, 'spears': 1940, 'great': 855, 'northern': 1339, 'rendered': 1665, 'goose': 829, 'candies': 261, 'sumac': 1980, 'stuffing': 1977, 'rutabagas': 1740, 'herbed': 926, 'pasilla': 1428, 'masa': 1206, 'harina': 904, 'beet': 93, 'arborio': 46, 'blender': 135, 'envelopes': 648, 'double': 588, 'strip': 1971, 'plastic': 1521, 'wood': 2128, 'briquettes': 202, 'metal': 1249, 'carcasses': 280, 'curd': 519, 'filling': 695, 'mildflavored': 1261, 'whiskey': 2105, 'color': 419, 'beefsteak': 91, 'pain': 1402, 'rustique': 1738, 'pimentn': 1495, 'vera': 2077, 'mche': 1219, 'brut': 216, 'flavoring': 726, 'tube': 2056, 'minus': 1268, 'young': 2151, 'concord': 431, 'rubber': 1731, 'gloves': 817, 'spareribs': 1935, 'szechuan': 1995, 'cardboard': 282, 'carton': 288, 'plumped': 1529, 'favorite': 680, 'crusts': 504, 'nut': 1343, 'bartlett': 75, 'rigatoni': 1688, 'pimientostuffed': 1498, 'brand': 183, 'hamburger': 893, 'canadian': 259, 'naam': 1306, 'pla': 1516, 'softleafed': 1917, 'scraped': 1785, 'poppy': 1544, 'figs': 690, 'calimyrna': 254, 'marrow': 1201, 'slow': 1904, 'cooker': 441, 'slender': 1895, 'york': 2149, 'rapini': 1631, 'leafy': 1079, 'collard': 416, 'earl': 620, 'grey': 861, 'noir': 1330, 'quince': 1616, 'venison': 2076, 'cook': 439, 'skins': 1890, 'percent': 1466, 'blanco': 132, 'reposado': 1666, 'passion': 1430, 'matzo': 1214, 'excluding': 661, 'liver': 1131, 'bouillon': 168, 'reserve': 1669, 'couscous': 468, 'pappardelle': 1415, 'julienned': 1016, 'bbq': 82, 'tamari': 2002, 'sloe': 1903, 'cajun': 247, 'fingerling': 701, 'russian': 1736, 'caviar': 300, 'bosc': 162, 'snap': 1908, 'dipping': 583, 'glasses': 813, 'penne': 1456, 'flatbreads': 719, 'jigger': 1008, 'oldfashioned': 1360, 'rolled': 1705, 'liner': 1116, 'silpat': 1873, 'candied': 260, 'clementines': 385, 'tangerines': 2005, 'wrapped': 2134, 'redleaf': 1649, 'refrigerator': 1657, 'papayas': 1411, 'littleneck': 1128, 'clams': 380, 'manila': 1186, 'slaw': 1894, 'backs': 61, 'necks': 1316, 'buttering': 239, 'capacity': 271, 'sift': 1870, 'kirby': 1042, 'racks': 1622, 'stew': 1958, 'chenin': 325, 'mission': 1273, 'campari': 258, 'simple': 1876, 'snow': 1911, 'pea': 1437, 'wholegrain': 2114, 'coarsegrain': 392, 'macadamia': 1158, 'medallions': 1227, 'buttered': 237, 'rolling': 1706, 'sorbet': 1921, 'process': 1578, 'mexicanstyle': 1252, 'lima': 1110, 'dust': 614, 'miso': 1272, 'seltzer': 1810, 'spelt': 1941, 'ducks': 609, 'wing': 2121, 'possible': 1555, 'pigs': 1492, 'foot': 748, 'feet': 682, 'kind': 1040, 'flattened': 723, 'frenchbread': 761, 'shoots': 1854, 'amaranth': 16, 'shank': 1832, 'aniseed': 32, 'curls': 521, 'fermented': 685, 'grassfed': 848, 'rimmed': 1689, 'flameproof': 713, 'glass': 812, 'passionfruit': 1431, 'pulp': 1592, 'tip': 2032, 'drizzling': 600, 'preserved': 1570, 'gochujang': 824, 'nori': 1338, 'turbinado': 2059, 'shortbread': 1857, 'makes': 1172, 'cakes': 249, 'dutch': 616, 'oat': 1348, 'reducedfat': 1652, 'sardines': 1763, 'bench': 99, 'tartlet': 2012, 'kidney': 1036, 'chardonnay': 318, 'live': 1130, 'lobsters': 1136, 'dog': 584, 'giblet': 804, 'fleur': 731, 'sel': 1808, 'hungarian': 957, 'quality': 1605, 'shells': 1848, 'hass': 907, 'chokes': 361, 'boysenberries': 177, 'jerusalem': 1006, 'sunchokes': 1982, 'tropical': 2052, 'granola': 839, 'tomatillo': 2039, 'husk': 958, 'cotija': 462, 'whiteskinned': 2110, 'flaxseeds': 729, 'loaves': 1134, 'anglaise': 29, 'turnip': 2064, 'collards': 417, 'sichuan': 1866, 'hanger': 898, 'kaffir': 1021, 'roma': 1708, 'madeira': 1163, 'harissa': 905, 'colander': 412, 'chopping': 363, 'cane': 263, 'hardshelled': 902, 'soba': 1912, 'rounded': 1727, 'blended': 134, 'bruised': 211, 'shaoxing': 1835, 'flatbottomed': 717, 'wok': 2125, 'readytouse': 1641, 'kelp': 1030, 'cloth': 386, 'frosting': 769, 'nam': 1308, 'daikon': 532, 'narrow': 1310, 'marsala': 1202, 'basic': 77, 'quickrising': 1612, 'vidalia': 2084, 'legthigh': 1091, 'deli': 558, 'counter': 465, 'chervil': 328, 'reconstituted': 1644, 'meringue': 1245, 'pin': 1499, 'sealable': 1791, 'icing': 965, 'millet': 1263, 'chestnut': 329, 'armagnac': 49, 'brine': 197, 'demiglace': 563, 'vegetarian': 2075, 'fusilli': 780, 'maytag': 1218, 'guanciale': 875, 'boilinghot': 152, 'mesclun': 1247, 'measured': 1223, 'flush': 738, 'doublepeeled': 591, 'muffins': 1295, 'littlenecks': 1129, 'summer': 1981, 'chambord': 311, 'oiling': 1356, 'skinon': 1889, 'chimichurri': 346, 'cottage': 463, 'beanthread': 85, 'fresno': 765, 'tendrils': 2021, 'better': 106, 'measure': 1222, 'paddle': 1400, 'cornish': 459, 'game': 785, 'hens': 924, 'quail': 1604, 'tagliatelle': 1998, 'pippin': 1507, 'sambal': 1757, 'oelek': 1352, 'hard': 899, 'grana': 836, 'padano': 1399, 'strozzapreti': 1975, 'flavor': 724, 'halfandhalf': 886, 'bundt': 226, 'minute': 1269, 'saltine': 1754, 'like': 1109, 'keeping': 1028, 'pareve': 1418, 'herbseasoned': 929, 'transfatfree': 2048, 'malted': 1178, 'walla': 2094, 'multigrain': 1296, 'cereal': 308, 'flax': 727, 'dozen': 593, 'labneh': 1058, 'lebanese': 1083, 'backbones': 60, 'mediterranean': 1228, 'instantespresso': 983, 'instantcoffee': 982, 'panko': 1408, 'jonagold': 1011, 'bias': 108, 'blossom': 138, 'clover': 388, 'original': 1380, 'sheepsmilk': 1842, 'tangy': 2006, 'flowerets': 736, 'blood': 137, 'orangeflower': 1372, 'chia': 331, 'xaand': 2136, 'morels': 1288, 'pack': 1393, 'galangal': 784, 'palm': 1405, 'jelly': 1005, 'pony': 1542, 'bndictine': 145, 'berry': 103, 'robust': 1700, 'blackstrap': 128, 'tuscan': 2066, 'lacinato': 1059, 'darkgreen': 539, 'farmers': 672, 'arctic': 47, 'char': 315, 'toast': 2034, 'wrap': 2133, 'prefer': 1566, 'tie': 2029, 'easier': 623, 'pectin': 1450, 'dulce': 610, 'cubanelle': 510, 'dumplings': 612, 'riesling': 1686, 'rhubarb': 1673, 'nonhydrogenated': 1333, 'vinaigrette': 2086, 'making': 1173, 'link': 1122, 'goats': 822, 'indonesian': 975, 'farm': 670, 'bass': 80, 'anchovies': 24, 'london': 1140, 'kernel': 1032, 'beards': 86, 'tenderloins': 2019, 'silver': 1874, 'membrane': 1242, 'aji': 7, 'striped': 1972, 'chanterelles': 314, 'bella': 97, 'joint': 1010, 'nice': 1325, 'creole': 485, 'freezer': 759, 'make': 1170, 'tapenade': 2007, 'lemonade': 1093, 'ale': 8, 'crisps': 488, 'farro': 674, 'fryer': 774, 'safflower': 1742, 'barley': 73, 'ear': 619, 'low': 1148, 'coarseground': 395, 'mace': 1161, 'rainbow': 1626, 'rotini': 1721, 'corkscrew': 452, 'peychauds': 1477, 'caramel': 276, 'corned': 456, 'lady': 1060, 'madras': 1164, 'filtered': 696, 'best': 104, 'stirred': 1966, 'loosen': 1146, 'framboise': 754, 'eaudevie': 627, 'driedxacrumbled': 598, 'roll': 1704, 'bodies': 148, 'savory': 1771, 'sake': 1746, 'molds': 1282, 'ziti': 2158, 'guacamole': 873, 'ramen': 1630, 'oxtails': 1389, 'dip': 582, 'poire': 1537, 'williams': 2119, 'yuca': 2152, 'crosscut': 493, 'pod': 1534, 'ceramic': 307, 'kikkoman': 1038, 'handle': 897, 'seafood': 1790, 'boil': 149, 'chartreuse': 320, 'seville': 1826, 'bitter': 120, 'rich': 1683, 'lambs': 1064, 'sushigrade': 1988, 'block': 136, 'lettuces': 1102, 'finger': 700, 'remainder': 1661, 'highquality': 935, 'leche': 1084, 'cassis': 295, 'moons': 1286, 'marzipan': 1205, 'coloring': 420, 'piquillo': 1508, 'converted': 438, 'count': 464, 'grind': 867, 'maldon': 1175, 'knob': 1050, 'merlot': 1246, 'porterhouse': 1550, 'chianti': 332, 'ovenproof': 1387, 'flavored': 725, 'place': 1517, 'allbutter': 10, 'grappa': 847, 'focaccia': 741, 'pretzels': 1575, 'cartilage': 287, 'stirfry': 1965, 'mcintosh': 1220, 'leftover': 1088, 'marinara': 1195, 'lemonpepper': 1095, 'big': 110, 'firstcut': 706, 'hickory': 932, 'semipearled': 1812, 'tapioca': 2008, 'pasillas': 1429, 'trout': 2053, 'spaghettini': 1933, 'ladyfingers': 1061, 'defrosted': 556, 'bitesized': 118, 'ingredients': 978, 'muscle': 1299, 'casserole': 294, 'ricer': 1679, 'dillweed': 580, 'quick': 1610, 'outside': 1384, 'remaining': 1662, 'brushed': 213, 'pop': 1543, 'darjeeling': 536, 'toffee': 2037, 'blackeyed': 127, 'rectangular': 1646, 'toasts': 2036, 'semiboneless': 1811, 'bottoms': 167, 'spear': 1938, 'fruity': 773, 'asiago': 54, 'piloncillo': 1493, 'grilled': 865, 'melonball': 1239, 'broccolini': 207, 'violets': 2088, 'doublecrust': 590, 'pekoe': 1455, 'eighths': 635, 'monkfish': 1283, 'gently': 799, 'diamond': 574, 'crystal': 506, 'resealable': 1668, 'lox': 1153, 'purple': 1600, 'glutenfree': 818, 'pollen': 1539, 'kept': 1031, 'mediumsized': 1235, 'canela': 264, 'deepdish': 552, 'plate': 1523, 'ahi': 6, 'applesauce': 40, 'colby': 413, 'completely': 426, 'rotelle': 1720, 'suet': 1978, 'yam': 2141, 'lined': 1115, 'quillshaped': 1615, 'high': 934, 'mallet': 1176, 'pattypan': 1436, 'chop': 362, 'rectangles': 1645, 'soppressata': 1920, 'provolone': 1584, 'pressure': 1574, 'niois': 1327, 'redcurrant': 1648, 'cashew': 290, 'aperol': 36, 'dungeness': 613, 'lavender': 1074, 'applejack': 38, 'gemelli': 796, 'stuck': 1976, 'meyer': 1254, 'compote': 427, 'skim': 1885, 'elbow': 636, 'macaroni': 1159, 'gherkin': 802, 'brush': 212, 'braeburn': 178, 'crudits': 500, 'roots': 1715, 'base': 76, 'orecchiette': 1375, 'pimiento': 1496, 'tentacles': 2022, 'muscovy': 1301, 'island': 989, 'pekin': 1454, 'hollow': 943, 'maine': 1167, 'marshmallow': 1203, 'creme': 483, 'open': 1367, 'ctes': 509, 'rhne': 1672, 'ricewine': 1682, 'brittle': 205, 'irish': 988, 'fatty': 678, 'biscuit': 114, 'mediumgrind': 1231, 'pullman': 1591, 'flatbread': 718, 'colmans': 418, 'farfalle': 669, 'gratin': 850, 'manchego': 1180, 'youll': 2150, 'osso': 1382, 'buco': 218, 'need': 1320, 'buckwheat': 217, 'rustic': 1737, 'rub': 1729, 'crystals': 508, 'commercial': 425, 'desiccated': 566, 'fenugreek': 684, 'rabbit': 1619, 'dumpling': 611, 'mandarin': 1181, 'flower': 735, 'cuminseed': 518, 'fashioned': 675, 'rotisserie': 1722, 'schnapps': 1780, 'eye': 668, 'garnishes': 792, 'prosecco': 1582, 'drop': 601, 'pliable': 1526, 'won': 2126, 'ton': 2043, 'chocolatecovered': 358, 'salami': 1748, 'genoa': 798, 'whitefleshed': 2108, 'picks': 1487, 'denver': 564, 'semolina': 1814, 'delicata': 559, 'achiote': 0, 'croissants': 491, 'intoinch': 987, 'brined': 199, 'biscuits': 115, 'pte': 1587, 'brise': 203, 'bar': 71, 'poached': 1531, 'belly': 98, 'farmer': 671, 'challah': 310, 'diluted': 581, 'wafers': 2092, 'beauty': 89, 'melons': 1240, 'peppadew': 1459, 'stgermain': 1960, 'elderflower': 637, 'mignon': 1258, 'rare': 1632, 'sercial': 1818, 'shot': 1860, 'lager': 1062, 'castelvetrano': 296, 'mediumhot': 1232, 'platter': 1525, 'sodium': 1914, 'scored': 1783, 'ready': 1640, 'menthe': 1244, 'plates': 1524, 'minimuffin': 1266, 'chill': 344, 'drain': 594, 'filets': 692, 'mignons': 1259, 'citron': 377, 'campanelle': 257, 'petit': 1475, 'length': 1097, 'israeli': 990, 'morton': 1290, 'similar': 1875, 'teriyaki': 2024, 'bird': 112, 'smithfield': 1907, 'hake': 884, 'mein': 1237, 'fatfree': 677, 'rasp': 1634, 'verjus': 2080, 'marshmallows': 1204, 'prime': 1576, 'sicilian': 1867, 'gutted': 879, 'kahla': 1022, 'tilapia': 2031, 'croutons': 496, 'fuyu': 781, 'persimmons': 1470, 'roomtemperature': 1713, 'bouquet': 169, 'lbs': 1077, 'securely': 1801, 'naan': 1307, 'dubonnet': 607, 'jarred': 1001, 'rigate': 1687, 'single': 1878, 'bit': 116, 'work': 2132, 'whiting': 2112, 'whisk': 2103, 'scooped': 1782, 'europeanstyle': 657, 'drippings': 599, 'loose': 1144, 'pimientos': 1497, 'form': 752, 'lardons': 1066, 'dandelion': 534, 'boursin': 171, 'fideos': 688, 'pulled': 1590, 'lasagna': 1069, 'partskim': 1427, 'peppered': 1462, 'mackerel': 1162, 'raselhanout': 1633, 'freerange': 757, 'pierced': 1491, 'fork': 751, 'area': 48, 'instructions': 985, 'fireroasted': 702, 'burritosize': 231, 'branzino': 186, 'bowtie': 174, 'shiitakes': 1851, 'ring': 1691, 'robustflavored': 1701, 'calabaza': 250, 'mountain': 1291, 'chipotles': 353, 'cobs': 398, 'muscles': 1300, 'pits': 1513, 'sixteen': 1880, 'chocolatexachopped': 359, 'peperoncini': 1457, 'caramelized': 277, 'request': 1667, 'butchers': 233, 'saltines': 1755, 'african': 3, 'flanken': 715, 'tubular': 2057, 'plantains': 1520, 'gaeta': 782, 'beurre': 107, 'mani': 1185, 'kneading': 1048, 'hispanic': 936, 'kiwis': 1047, 'benedictine': 100, 'limeade': 1112, 'taco': 1997, 'okra': 1358, 'blackcurrantflavored': 126, 'quills': 1614, 'shaped': 1837, 'butterscotch': 242, 'hersheys': 930, 'chunk': 368, 'perugina': 1471, 'bulk': 223, 'nectarine': 1318, 'handfuls': 896, 'poussins': 1561, 'jiggers': 1009, 'ponies': 1541, 'segment': 1806, 'skor': 1892, 'raisin': 1627, 'carcass': 279, 'farms': 673, 'bars': 74, 'oval': 1385, 'recipes': 1643, 'porter': 1549, 'puy': 1603, 'coffeeflavored': 406, 'partially': 1425, 'blowtorch': 140, 'premium': 1568, 'coarsegrind': 394, 'san': 1759, 'lingonberry': 1118, 'punch': 1595, 'pudding': 1588, 'octopus': 1351, 'idaho': 966, 'mexicana': 1251, 'dogs': 585, 'maitake': 1168, 'heath': 919, 'combined': 423, 'lightgreen': 1107, 'creamed': 479, 'enchilada': 640, 'backbone': 59, 'finegrain': 698, 'vegetableoil': 2073, 'microgreens': 1255, 'feathers': 681, 'needlenose': 1322, 'pliers': 1527, 'containing': 437, 'page': 1401, 'crostini': 495, 'emmenthal': 639, 'zaatar': 2154, 'amarillo': 19, 'whisked': 2104, 'crpes': 499, 'tel': 2016, 'woody': 2130, 'cores': 450, 'currantflavored': 524, 'rome': 1711, 'microplane': 1256, 'sheeps': 1841, 'logs': 1138, 'comt': 428, 'turbot': 2060, 'perciatelli': 1467, 'kefalotyri': 1029, 'opened': 1368, 'smallcurd': 1905, 'peels': 1453, 'precooked': 1565, 'marinade': 1194, 'jamaican': 997, 'refried': 1655, 'cipolline': 376, 'depending': 565, 'heat': 917, 'seasonings': 1795, 'portuguese': 1554, 'gem': 795, 'nutella': 1344, 'braising': 180, 'balm': 66, 'fullbodied': 777, 'hold': 940, 'earshaped': 622, 'braised': 179, 'moist': 1279, 'decorating': 548, 'carnaroli': 284, 'crispy': 489, 'tostada': 2047, 'wakame': 2093, 'truffles': 2055, 'nutritional': 1346, 'largecurd': 1067, 'garni': 790, 'separate': 1815, 'carefully': 283, 'tortellini': 2044, 'super': 1985, 'bordeaux': 161, 'delicate': 560, 'sandwiches': 1762, 'crabs': 473, 'brioches': 201, 'danish': 535, 'shiso': 1853, 'gochugaru': 823, 'chine': 348, 'ligament': 1104, 'foamy': 740, 'hash': 906, 'treviso': 2049, 'creamstyle': 480, 'corkscrewshaped': 453, 'aquavit': 44, 'wholeberry': 2113, 'chinesestyle': 350, 'roundbone': 1726, 'piment': 1494, 'despelette': 568, 'saltpacked': 1756, 'lower': 1149, 'xainto': 2137, 'spearmint': 1939, 'grinds': 869, 'glazed': 815, 'petals': 1474, 'buttercream': 236, 'holland': 942, 'matcha': 1210, 'chickenapple': 334, 'chayote': 321, 'shiro': 1852, 'lightcolored': 1106, 'muenster': 1293, 'globe': 816, 'biscotti': 113, 'matchstick': 1211, 'choice': 360, 'hickorysmoked': 933, 'arbol': 45, 'liners': 1117, 'east': 624, 'shaohsing': 1834, 'scallop': 1777, 'ravioli': 1637, 'bluefish': 144, 'following': 743, 'shaker': 1827, 'sixths': 1881, 'burgundy': 228, 'purslane': 1602, 'prawns': 1564, 'japanesestyle': 999, 'calamari': 252, 'hen': 923, 'cockles': 399, 'shape': 1836, 'crowns': 498, 'lychees': 1157, 'mesquite': 1248, 'icecold': 963, 'bestquality': 105, 'manis': 1187, 'cornbread': 455, 'shanghai': 1831, 'sauted': 1768, 'savoiardi': 1770, 'manual': 1189, 'valrhona': 2068, 'garlicflavored': 789, 'crown': 497, 'halve': 889, 'hibiscus': 931, 'ritz': 1695, 'hand': 894, 'pineapples': 1503, 'decoration': 549}\n",
      "2160\n",
      "<class 'numpy.ndarray'>\n",
      "2160\n"
     ]
    }
   ],
   "source": [
    "# now we have the vocabulary\n",
    "print(type(vectorizer.vocabulary_))\n",
    "print(vectorizer.vocabulary_)\n",
    "print(len(vectorizer.vocabulary_))\n",
    "print(type(vectorizer.get_feature_names_out()))\n",
    "print(len(vectorizer.get_feature_names_out()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('oil', 8486), ('salt', 7708), ('sugar', 7120), ('butter', 5576), ('olive', 5399), ('pepper', 5304), ('juice', 5078), ('garlic', 4726), ('lemon', 4256), ('red', 3793), ('cream', 3782), ('cloves', 3344), ('leaves', 3307), ('flour', 3239), ('onion', 3045), ('pieces', 2991), ('chicken', 2817), ('vinegar', 2649), ('freshly', 2598), ('stick', 2481), ('cheese', 2456), ('vegetable', 2445), ('green', 2260), ('dry', 2244), ('wine', 2231), ('packed', 2228), ('coarsely', 2131), ('egg', 2109), ('parsley', 2058), ('kosher', 2042), ('slices', 2030), ('sauce', 2013), ('halved', 1988), ('eggs', 1971), ('broth', 1899), ('powder', 1843), ('orange', 1823), ('vanilla', 1804), ('tomatoes', 1734), ('seeds', 1710), ('milk', 1668), ('lime', 1640), ('onions', 1618), ('drained', 1570), ('extravirgin', 1519), ('thyme', 1477), ('allpurpose', 1466), ('ginger', 1401), ('brown', 1366), ('lengthwise', 1351)]\n"
     ]
    }
   ],
   "source": [
    "def printMostFrequentWords():\n",
    "    # this function will sort the vocab and print the 50 most frequent words\n",
    "    sum_words = ingredients_matrix.sum(axis = 0)\n",
    "    words_freq = [(word, sum_words[0, i]) for word, i in vectorizer.vocabulary_.items()]\n",
    "    words_freq = sorted(words_freq, key = lambda x: x[1], reverse = True)\n",
    "    # comment the above line and uncomment below to see least frequent words\n",
    "    # words_freq = sorted(words_freq, key = lambda x: x[1])\n",
    "    print(words_freq[0:50])\n",
    "\n",
    "# call the function\n",
    "printMostFrequentWords()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the vocab to a file\n",
    "# with open(\"ingvect\", \"w\") as outfile:\n",
    "#     outfile.write(\"\\n\".join(vectorizer.get_feature_names_out()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TODO: THIS STEP IS MANUAL, DO IT PROGRAMATICALLY\n",
    "# # Load the updated vocab to a list\n",
    "# new_vocab_list = None\n",
    "# with open('ingvect_mod') as f:\n",
    "#     new_vocab_list = f.read().splitlines()\n",
    "# len(new_vocab_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2161"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_vocab_list = list(vectorizer.get_feature_names_out())\n",
    "new_vocab_list.append(\"Peanut butter\")\n",
    "len(new_vocab_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's set this new vocab as a hyperparameter of the vectorizer\n",
    "vectorizer_mod = CountVectorizer(tokenizer=(cust_tokenizer.my_tokenizer),\n",
    "                       min_df=5, vocabulary=new_vocab_list)\n",
    "# not calling fit because sending custom vocab..\n",
    "ingredients_matrix_mod = vectorizer_mod.transform(df['ingredientsStr'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding a function to print the list of ingredients given a recipe title\n",
    "def printIngredients(recipeName):\n",
    "    sel = df['title'] == recipeName\n",
    "    print(recipeName)\n",
    "    print(df.loc[sel, ['ingredientsStr']].values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# model = NearestNeighbors(n_neighbors=11, metric='cosine')\n",
    "# model.fit(ingredients_matrix)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>NearestNeighbors(metric=&#x27;cosine&#x27;, n_neighbors=11)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">NearestNeighbors</label><div class=\"sk-toggleable__content\"><pre>NearestNeighbors(metric=&#x27;cosine&#x27;, n_neighbors=11)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "NearestNeighbors(metric='cosine', n_neighbors=11)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "model = NearestNeighbors(n_neighbors=11, metric='cosine')\n",
    "model.fit(ingredients_matrix_mod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Input ingredients: Peanut butter\n",
      "\n",
      " Result\n",
      "To Clarify Butter  :  0.293  :  5.0\n",
      "Peanut Butter and Banana Sandwiches  :  0.36  :  3.75\n",
      "Peanut Butter Cheesecake with Peanut Brittle  :  0.411  :  3.75\n",
      "Peanut Butter and Jelly Layered Sandwiches  :  0.423  :  4.375\n",
      "Peanut Punch  :  0.423  :  0.0\n",
      "Peanut Butter, Banana, and Jelly \"Ice Cream\"  :  0.465  :  0.0\n",
      "Shallot Butter  :  0.5  :  5.0\n",
      "Whole-Wheat Peanut Butter Waffles  :  0.513  :  3.75\n",
      "Milk Chocolate Peanut Butter Sauce  :  0.513  :  2.5\n",
      "Easy Crepes  :  0.529  :  5.0\n",
      "Peanut Butter Chocolate Ripple Ice Cream  :  0.529  :  4.375\n"
     ]
    }
   ],
   "source": [
    "# Using various ingredient lists to test the results\n",
    "\n",
    "ingInputList = [\n",
    "    # \"Chicken, Parmesan, Breadcrumbs\",  # something familiar\n",
    "    # \"Artichoke Pesto\",\n",
    "    # \"Chicken thighs, potatoes\",  # compare results of potatoes vs potato\n",
    "    # \"Chicken thighs, potato\",\n",
    "    # \"Okra\",  # single ingredient\n",
    "    # \"Bhindi\",  # unknown ingredient - does not exist in the vocabulary\n",
    "    \"Peanut butter\"  # This is a 2 word ingredient\n",
    "]\n",
    "\n",
    "for ingInput in ingInputList:\n",
    "    print(f\"\\n Input ingredients: {ingInput}\")\n",
    "    # Convert the string to a series\n",
    "    ingInputSeries = pd.Series(ingInput)\n",
    "\n",
    "    # Let's try to use the vectorizer on this\n",
    "    ingTransformed = vectorizer_mod.transform(ingInputSeries)\n",
    "\n",
    "    # pass this to NearestNeighbors trained model\n",
    "    distOfRes, indicesOfRes = model.kneighbors(ingTransformed)\n",
    "\n",
    "    # print the output\n",
    "    print(\"\\n Result\")\n",
    "\n",
    "    for i in range(0, 11):  # TODO: 11 should be made configurable and match the n-neighbors number\n",
    "        name = df.loc[indicesOfRes[0][i], ['title']].values[0]\n",
    "        distance = (distOfRes[0][i]).round(3)\n",
    "        rating = df.loc[indicesOfRes[0][i], ['rating']].values[0]\n",
    "\n",
    "        # print(f\"{name}  :  {distance}\")\n",
    "        print(f\"{name}  :  {distance}  :  {rating}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To Clarify Butter\n",
      "[[\"['Unsalted butter']\"]]\n",
      "Peanut Punch\n",
      "[[\"['2 tablespoons cornstarch', '1/2 cup water', '2 cups milk', '6 tablespoons peanut butter', 'Sugar to taste']\"]]\n"
     ]
    }
   ],
   "source": [
    "printIngredients(\"To Clarify Butter\")\n",
    "printIngredients(\"Peanut Punch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[1]], dtype=int64)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recipe_index = df[df['title'] == 'To Clarify Butter'].index\n",
    "ingredients_matrix_mod[recipe_index].sum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most frequent bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\anami\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Get bigrams out and see the most common ones\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vect_bg = CountVectorizer(tokenizer=(cust_tokenizer.my_tokenizer),\n",
    "                       min_df=5, ngram_range=(2,2))\n",
    "ing_bg_mat = vect_bg.fit_transform(df['ingredientsStr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('olive oil', 5344), ('garlic cloves', 2617), ('lemon juice', 2415), ('kosher salt', 2008), ('vegetable oil', 2001), ('stick butter', 1901), ('freshly pepper', 1546), ('extravirgin olive', 1473), ('allpurpose flour', 1457), ('chicken broth', 1432), ('brown sugar', 1245), ('purpose flour', 1192), ('whipping cream', 1103), ('salt pepper', 1090), ('lime juice', 1056), ('salt freshly', 1030), ('vanilla extract', 989), ('heavy cream', 953), ('garlic clove', 894), ('dry wine', 868), ('lowsalt chicken', 813), ('bell pepper', 736), ('soy sauce', 730), ('dijon mustard', 716), ('wine vinegar', 707), ('red wine', 703), ('sour cream', 688), ('red onion', 671), ('red bell', 665), ('butter melted', 661), ('red pepper', 660), ('orange juice', 636), ('butter room', 627), ('halved lengthwise', 615), ('egg yolks', 592), ('lemon peel', 578), ('inchthick slices', 568), ('sea salt', 557), ('green onions', 538), ('parsley leaves', 526), ('sticks butter', 498), ('butter pieces', 495), ('parmesan cheese', 482), ('flatleaf parsley', 476), ('lemon zest', 471), ('sugar salt', 451), ('granulated sugar', 449), ('canned lowsalt', 448), ('balsamic vinegar', 447), ('oil onion', 431)]\n"
     ]
    }
   ],
   "source": [
    "def printMostFrequentBigrams():\n",
    "    # this function will sort the vocab and print the 50 most frequent words\n",
    "    sum_words = ing_bg_mat.sum(axis = 0)\n",
    "    words_freq = [(word, sum_words[0, i]) for word, i in vect_bg.vocabulary_.items()]\n",
    "    words_freq = sorted(words_freq, key = lambda x: x[1], reverse = True)\n",
    "    # comment the above line and uncomment below to see least frequent words\n",
    "    # words_freq = sorted(words_freq, key = lambda x: x[1])\n",
    "    print(words_freq[0:50])\n",
    "\n",
    "# call the function\n",
    "printMostFrequentBigrams()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6035"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect_bg.vocabulary_['peanut butter']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5462"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect_bg.vocabulary_['olive oil']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorted_vocab = sorted(vect_bg.vocabulary_.values(), reverse=True)\n",
    "sorted_vocab = sorted(vect_bg.vocabulary_.items(), key=lambda x:x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('yolks sugar', 9656),\n",
       " ('yolks sticks', 9655),\n",
       " ('yolks stick', 9654),\n",
       " ('yolks sour', 9653),\n",
       " ('yolks salt', 9652),\n",
       " ('yolks packed', 9651),\n",
       " ('yolks orange', 9650),\n",
       " ('yolks milk', 9649),\n",
       " ('yolks lime', 9648),\n",
       " ('yolks light', 9647),\n",
       " ('yolks lemon', 9646),\n",
       " ('yolks honey', 9645),\n",
       " ('yolks heavy', 9644),\n",
       " ('yolks granulated', 9643),\n",
       " ('yolks extravirgin', 9642),\n",
       " ('yolks equipment', 9641),\n",
       " ('yolks eggs', 9640),\n",
       " ('yolks egg', 9639),\n",
       " ('yolks cornstarch', 9638),\n",
       " ('yolks chilled', 9637),\n",
       " ('yolks butter', 9636),\n",
       " ('yolks beaten', 9635),\n",
       " ('yolks allpurpose', 9634),\n",
       " ('yolk whipping', 9633),\n",
       " ('yolk vanilla', 9632),\n",
       " ('yolk sugar', 9631),\n",
       " ('yolk purpose', 9630),\n",
       " ('yolk milk', 9629),\n",
       " ('yolk lightly', 9628),\n",
       " ('yolk ice', 9627),\n",
       " ('yolk heavy', 9626),\n",
       " ('yolk equipment', 9625),\n",
       " ('yolk beaten', 9624),\n",
       " ('yogurt vegetable', 9623),\n",
       " ('yogurt vanilla', 9622),\n",
       " ('yogurt sugar', 9621),\n",
       " ('yogurt sour', 9620),\n",
       " ('yogurt serving', 9619),\n",
       " ('yogurt salt', 9618),\n",
       " ('yogurt preferably', 9617),\n",
       " ('yogurt packed', 9616),\n",
       " ('yogurt olive', 9615),\n",
       " ('yogurt mint', 9614),\n",
       " ('yogurt milk', 9613),\n",
       " ('yogurt mayonnaise', 9612),\n",
       " ('yogurt lowfat', 9611),\n",
       " ('yogurt lime', 9610),\n",
       " ('yogurt lemon', 9609),\n",
       " ('yogurt kosher', 9608)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_vocab[51:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selecting common bigrams\n",
    "\n",
    "chicken broth  \n",
    "bell pepper  \n",
    "soy sauce  \n",
    "\n",
    "Not necessary to specify them actually. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 680 custom vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "668"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_680 = None\n",
    "with open('ing680') as f:\n",
    "    vocab_680 = f.read().splitlines()\n",
    "len(vocab_680)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>NearestNeighbors(metric=&#x27;cosine&#x27;, n_neighbors=11)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">NearestNeighbors</label><div class=\"sk-toggleable__content\"><pre>NearestNeighbors(metric=&#x27;cosine&#x27;, n_neighbors=11)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "NearestNeighbors(metric='cosine', n_neighbors=11)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's set this new vocab as a hyperparameter of the vectorizer\n",
    "vect_680 = CountVectorizer(tokenizer=(cust_tokenizer.my_tokenizer),\n",
    "                       min_df=5, vocabulary=vocab_680)\n",
    "# not calling fit because sending custom vocab..\n",
    "ingredients_matrix_680 = vect_680.transform(df['ingredientsStr'])\n",
    "\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "model_680 = NearestNeighbors(n_neighbors=11, metric='cosine')\n",
    "model_680.fit(ingredients_matrix_680)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Input ingredients: Chicken, Parmesan, Breadcrumbs\n",
      "\n",
      " Result\n",
      "Parmesan Chicken with Mixed Baby Greens  :  0.304  :  4.375\n",
      "Chicken Soup Verde  :  0.388  :  3.125\n",
      "Breaded Chicken Cutlets with Chunky Vegetable Sauce  :  0.404  :  3.75\n",
      "Linguine with Sausage, Mushroom and Cream Sauce  :  0.423  :  3.75\n",
      "Chicken Breasts with Sun-Dried Tomato and Garlic Crust  :  0.423  :  3.75\n",
      "Lemon-Pepper Chicken  :  0.423  :  4.375\n",
      "Chicken Divan  :  0.423  :  3.75\n",
      "Creamed Mushrooms, Onions, and Brussels Sprouts  :  0.423  :  4.375\n",
      "Parmesan Polenta  :  0.423  :  4.375\n",
      "Root Vegetable Gratin  :  0.452  :  4.375\n",
      "Suzanne's Scalloppine  :  0.452  :  3.75\n",
      "\n",
      " Input ingredients: Artichoke Pesto\n",
      "\n",
      " Result\n",
      "Artichoke and Olive Crostini  :  0.423  :  3.75\n",
      "Artichoke Hearts with Garlic, Olive Oil and Parsley  :  0.5  :  3.125\n",
      "Hot Artichoke and Tarragon Dip  :  0.5  :  3.75\n",
      "Baked Chicken with Mushrooms and Artichokes  :  0.5  :  3.75\n",
      "Grilled Fontina with Artichokes and Mushrooms  :  0.5  :  3.125\n",
      "Roasted Pacific Cod with Spring Vegetables and Mint  :  0.553  :  4.375\n",
      "Artichoke Croustades  :  0.553  :  3.75\n",
      "Artichoke Bruschetta  :  0.553  :  4.375\n",
      "Antipasto-Stuffed Baguettes  :  0.553  :  4.375\n",
      "Artichoke, Cherry Tomato, and Feta Salad with Artichoke-Pesto Crostini  :  0.574  :  4.375\n",
      "Linguine Avgolemono with Artichoke Hearts and Green Beans  :  0.592  :  3.75\n",
      "\n",
      " Input ingredients: Chicken thighs, potatoes\n",
      "\n",
      " Result\n",
      "Lemon-Pepper Chicken  :  0.0  :  4.375\n",
      "Chicken Soup  :  0.085  :  3.75\n",
      "Southern-Style Fried Chicken  :  0.095  :  0.0\n",
      "Roast Chicken with Pesto and Potatoes  :  0.106  :  4.375\n",
      "Chicken, Vegetable, and Dumpling Soup  :  0.168  :  3.125\n",
      "Horseradish Mashed Potatoes  :  0.184  :  3.75\n",
      "Roasted Chicken, Ramps, and Potatoes  :  0.184  :  4.375\n",
      "Iroquois Stew with Beef, Chicken and Pork  :  0.184  :  3.125\n",
      "Green Chile Chicken Tamales  :  0.184  :  5.0\n",
      "Chicken and Mixed Vegetable Stew  :  0.184  :  4.375\n",
      "Italian Chicken Soup  :  0.184  :  3.125\n",
      "\n",
      " Input ingredients: Chicken thighs, potato\n",
      "\n",
      " Result\n",
      "Chicken Vegetable Soup with Ginger  :  0.244  :  3.75\n",
      "Chicken and Corn Chowder with Thyme  :  0.293  :  3.75\n",
      "Lemon-Pepper Chicken  :  0.293  :  4.375\n",
      "Passover Powdered Sugar  :  0.293  :  0.0\n",
      "Chicken Soup  :  0.353  :  3.75\n",
      "Southern-Style Fried Chicken  :  0.36  :  0.0\n",
      "Cold Garlic Potato Soup  :  0.368  :  3.125\n",
      "Split Pea Soup with Sausage and Potato  :  0.368  :  4.375\n",
      "Turkey Sage Chowder  :  0.368  :  4.375\n",
      "Sorrel, Pea, and Leek Soup  :  0.368  :  3.75\n",
      "Corn and Bell Pepper Chowder  :  0.368  :  4.375\n",
      "\n",
      " Input ingredients: Okra\n",
      "\n",
      " Result\n",
      "Okra with Scallion, Lime, and Ginger  :  0.5  :  3.75\n",
      "Stewed Corn and Tomatoes with Okra  :  0.5  :  4.375\n",
      "Okra Beignets with Cilantro Sour Cream Sauce  :  0.583  :  3.75\n",
      "Sauteed Okra with Tomato and Corn  :  0.592  :  4.375\n",
      "Chicken, Sausage, and Okra Gumbo  :  0.608  :  2.5\n",
      "Broiled Tomato, Corn, and Okra  :  0.622  :  4.375\n",
      "Succotash  :  0.667  :  4.375\n",
      "Creole Chicken and Okra Gumbo  :  0.667  :  3.75\n",
      "Chive Shortcakes with Smoky Corn and Okra Stew  :  0.667  :  3.125\n",
      "Shrimp and Andouille Sausage Gumbo  :  0.684  :  4.375\n",
      "Catfish and Okra with Pecan Butter Sauce  :  0.684  :  4.375\n",
      "\n",
      " Input ingredients: Bhindi\n",
      "\n",
      " Result\n",
      "Sour Cream Chocolate Cake  :  1.0  :  3.75\n",
      "Zucchini-Blossom Quesadillas  :  1.0  :  3.75\n",
      "Summer Seafood Stew  :  1.0  :  4.375\n",
      "Baked Sea Bass with Walnut-Breadcrumb Crust and Lemon-Dill Sauce  :  1.0  :  4.375\n",
      "Maple-Walnut Espresso Torte  :  1.0  :  3.75\n",
      "Turkey Cheddar Sandwiches with Honey Mustard  :  1.0  :  3.75\n",
      "Poached Eggs in Pipérade  :  1.0  :  4.375\n",
      "Farfalle with Tomatoes and Feta Cheese  :  1.0  :  4.375\n",
      "Potato, Cucumber and Dill Salad  :  1.0  :  4.375\n",
      "Chocolate and Mixed Nut Tart in Cookie Crust  :  1.0  :  4.375\n",
      "Scallop Potatoes with Gouda and Fennel  :  1.0  :  4.375\n",
      "\n",
      " Input ingredients: Peanut butter\n",
      "\n",
      " Result\n",
      "Peanut Punch  :  0.0  :  0.0\n",
      "Peanut Butter and Banana Sandwiches  :  0.134  :  3.75\n",
      "Peanut Butter Cheesecake with Peanut Brittle  :  0.143  :  3.75\n",
      "Peanut Butter, Banana, and Jelly \"Ice Cream\"  :  0.184  :  0.0\n",
      "3-Ingredient Peanut Butter Cookies  :  0.184  :  5.0\n",
      "Peanut Butter Cheesecake with Caramelized Banana Topping  :  0.184  :  3.75\n",
      "Peanut Butter Crème Brûlée  :  0.184  :  1.875\n",
      "Peanut Butter Banana Ice Cream  :  0.198  :  3.125\n",
      "Chocolate-Peanut Butter Cake with Cream Cheese and Butterfinger Frosting  :  0.198  :  4.375\n",
      "Whole-Wheat Peanut Butter Waffles  :  0.198  :  3.75\n",
      "Peanut Butter Chocolate Chip Breads  :  0.198  :  3.75\n"
     ]
    }
   ],
   "source": [
    "# Using various ingredient lists to test the results\n",
    "\n",
    "ingInputList = [\n",
    "    \"Chicken, Parmesan, Breadcrumbs\",  # something familiar\n",
    "    \"Artichoke Pesto\",\n",
    "    \"Chicken thighs, potatoes\",  # compare results of potatoes vs potato\n",
    "    \"Chicken thighs, potato\",\n",
    "    \"Okra\",  # single ingredient\n",
    "    \"Bhindi\",  # unknown ingredient - does not exist in the vocabulary\n",
    "    \"Peanut butter\"  # This is a 2 word ingredient\n",
    "]\n",
    "\n",
    "for ingInput in ingInputList:\n",
    "    print(f\"\\n Input ingredients: {ingInput}\")\n",
    "    # Convert the string to a series\n",
    "    ingInputSeries = pd.Series(ingInput)\n",
    "\n",
    "    # Let's try to use the vectorizer on this\n",
    "    ingTransformed = vect_680.transform(ingInputSeries)\n",
    "\n",
    "    # pass this to NearestNeighbors trained model\n",
    "    distOfRes, indicesOfRes = model_680.kneighbors(ingTransformed)\n",
    "\n",
    "    # print the output\n",
    "    print(\"\\n Result\")\n",
    "\n",
    "    for i in range(0, 11):  # TODO: 11 should be made configurable and match the n-neighbors number\n",
    "        name = df.loc[indicesOfRes[0][i], ['title']].values[0]\n",
    "        distance = (distOfRes[0][i]).round(3)\n",
    "        rating = df.loc[indicesOfRes[0][i], ['rating']].values[0]\n",
    "\n",
    "        # print(f\"{name}  :  {distance}\")\n",
    "        print(f\"{name}  :  {distance}  :  {rating}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorrel, Pea, and Leek Soup\n",
      "[[\"['white and pale green parts of 3 leeks (about 3/4 pound), chopped, washed well, and drained', '1 1/2 tablespoons olive oil', '1 small boiling potato (about 1/4 pound)', '1 1/2 cups chicken broth', '1 1/2 cups cold water plus additional to thin soup', '1/2 cup shelled fresh or thawed frozen peas', '1/4 pound sorrel*, stems discarded and leaves washed, spun dry, and cut crosswise into thin strips (about 3 cups loosely packed)', '1/3 cup sour cream', '1 teaspoon fresh lemon juice, or to taste', 'Garnish: chopped hard-boiled egg and thin strips of sorrel', 'available seasonally at some supermarkets and specialty produce markets']\"]]\n"
     ]
    }
   ],
   "source": [
    "printIngredients(\"Sorrel, Pea, and Leek Soup\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spell Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "broccoli\n",
      "mozzarella\n",
      "cheese\n",
      "ladyfingers\n",
      "brine\n",
      "tomatoes\n",
      "potatoes\n",
      "tomatoes\n",
      "fish\n",
      "egg\n",
      "octopus\n",
      "oyster\n",
      "feta\n"
     ]
    }
   ],
   "source": [
    "# spell checker - https://www.geeksforgeeks.org/correcting-words-using-nltk-in-python/\n",
    "# importing the nltk suite\n",
    "import nltk\n",
    "\n",
    "# importing jaccard distance\n",
    "# and ngrams from nltk.util\n",
    "from nltk.metrics.distance import jaccard_distance\n",
    "from nltk.util import ngrams\n",
    "\n",
    "correct_words = vectorizer.get_feature_names_out()\n",
    "\n",
    "# that need to be corrected\n",
    "incorrect_words=['brocolli', 'mozarela', 'cheese', 'ladyfinger', 'brinjal', 'tomatoe', 'potatoes',\n",
    "'tomatoe','fish','egg','octopuses','oyster','fetas']\n",
    "\n",
    "# loop for finding correct spellings\n",
    "# based on jaccard distance\n",
    "# and printing the correct word\n",
    "for word in incorrect_words:\n",
    "    temp = [(jaccard_distance(set(ngrams(word, 2)),\n",
    "                              set(ngrams(w, 2))),w)\n",
    "            for w in correct_words if w[0]==word[0]]\n",
    "    print(sorted(temp, key = lambda val:val[0])[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorry we could not find recipes with the ingredient octopodes\n",
      "Sorry we could not find recipes with the ingredient fetum\n",
      "['tomato', 'oysters', 'potato', 'feta', 'eggs', 'oyster', 'octopus', 'tomatoes', 'egg', 'potatoes', 'fish']\n"
     ]
    }
   ],
   "source": [
    "# spell checker - https://www.geeksforgeeks.org/correcting-words-using-nltk-in-python/\n",
    "# importing the nltk suite\n",
    "import nltk\n",
    "\n",
    "# importing jaccard distance\n",
    "# and ngrams from nltk.util\n",
    "from nltk.metrics.distance import jaccard_distance\n",
    "from nltk.util import ngrams\n",
    "\n",
    "correct_words = vectorizer.get_feature_names_out() # or just put vocab variable here\n",
    "\n",
    "# that need to be corrected\n",
    "# incorrect_words=['brocolli', 'mozarela', 'cheese', 'ladyfinger', 'brinjal', 'tomatoe', 'potatoes','fish','egg','octopi','oyster','fetas']\n",
    "incorrect_words = ['tomato', 'oysterss', 'potato', 'octopodes', 'fetas', 'eggss', 'oyster', 'octopu', 'tomatoess', 'egg', 'potatoes', 'fetum', 'fish']\n",
    "# incorrect_words=['fetas']\n",
    "# incorrect_words=['cheese']\n",
    "\n",
    "# loop for finding correct spellings\n",
    "# based on jaccard distance\n",
    "# and printing the correct word\n",
    "\n",
    "final_words = []\n",
    "for word in incorrect_words:\n",
    "    temp = [(jaccard_distance(set(ngrams(word, 2)),\n",
    "                              set(ngrams(w, 2))),w)\n",
    "            for w in correct_words if w[0]==word[0]]\n",
    "    sorted_temp = sorted(temp, key = lambda val:val[0])\n",
    "    word_distance = sorted_temp[0][0]\n",
    "    corrected_word = sorted_temp[0][1]\n",
    "    # print(word_distance)\n",
    "    # print(corrected_word)\n",
    "    if word_distance > 0.5:\n",
    "        print(f\"Sorry we could not find recipes with the ingredient {word}\")\n",
    "    else:\n",
    "        final_words.append(corrected_word)\n",
    "print(final_words)\n",
    "    # print(sorted(temp, key = lambda val:val[0])[0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "words = [\"potato\", \"tomatoes\", \"fish\", \"eggs\", \"octopus\", \"oysters\", \"feta\"]\n",
    "\n",
    "I like the way the nltk works better than inflect even if it requires more setup."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plurals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using `inflect` package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plurals\n",
    "# %pip install inflect\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "potatoes\n",
      "tomatoe\n",
      "fish\n",
      "egg\n",
      "octopuses\n",
      "oyster\n",
      "fetas\n"
     ]
    }
   ],
   "source": [
    "import inflect\n",
    "\n",
    "p = inflect.engine()\n",
    "res = []\n",
    "words = [\"potato\", \"tomatoes\", \"fish\", \"eggs\", \"octopus\", \"oysters\", \"feta\"]\n",
    "for word in words:\n",
    "    print(p.plural(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using `nltk` and `pattern-en` package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pattern\n",
      "  Downloading Pattern-3.6.0.tar.gz (22.2 MB)\n",
      "     ---------------------------------------- 0.0/22.2 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/22.2 MB ? eta -:--:--\n",
      "     --------------------------------------- 0.0/22.2 MB 393.8 kB/s eta 0:00:57\n",
      "     ---------------------------------------- 0.2/22.2 MB 1.8 MB/s eta 0:00:13\n",
      "     - -------------------------------------- 1.0/22.2 MB 5.6 MB/s eta 0:00:04\n",
      "     ---- ----------------------------------- 2.6/22.2 MB 11.2 MB/s eta 0:00:02\n",
      "     -------- ------------------------------- 4.5/22.2 MB 16.0 MB/s eta 0:00:02\n",
      "     ----------- ---------------------------- 6.6/22.2 MB 20.1 MB/s eta 0:00:01\n",
      "     --------------- ------------------------ 8.4/22.2 MB 22.4 MB/s eta 0:00:01\n",
      "     ------------------ -------------------- 10.8/22.2 MB 38.5 MB/s eta 0:00:01\n",
      "     -------------------- ------------------ 12.0/22.2 MB 38.5 MB/s eta 0:00:01\n",
      "     ------------------------ -------------- 14.0/22.2 MB 43.7 MB/s eta 0:00:01\n",
      "     --------------------------- ----------- 15.5/22.2 MB 40.9 MB/s eta 0:00:01\n",
      "     ------------------------------- ------- 17.7/22.2 MB 43.5 MB/s eta 0:00:01\n",
      "     ---------------------------------- ---- 19.8/22.2 MB 43.5 MB/s eta 0:00:01\n",
      "     --------------------------------------  22.0/22.2 MB 43.5 MB/s eta 0:00:01\n",
      "     --------------------------------------  22.2/22.2 MB 43.7 MB/s eta 0:00:01\n",
      "     --------------------------------------- 22.2/22.2 MB 36.4 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: future in c:\\users\\anami\\anaconda3\\lib\\site-packages (from pattern) (0.18.3)\n",
      "Collecting backports.csv (from pattern)\n",
      "  Downloading backports.csv-1.0.7-py2.py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting mysqlclient (from pattern)\n",
      "  Downloading mysqlclient-2.2.4-cp311-cp311-win_amd64.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\anami\\anaconda3\\lib\\site-packages (from pattern) (4.12.2)\n",
      "Requirement already satisfied: lxml in c:\\users\\anami\\anaconda3\\lib\\site-packages (from pattern) (4.9.3)\n",
      "Collecting feedparser (from pattern)\n",
      "  Downloading feedparser-6.0.11-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting pdfminer.six (from pattern)\n",
      "  Downloading pdfminer.six-20231228-py3-none-any.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\anami\\anaconda3\\lib\\site-packages (from pattern) (1.26.4)\n",
      "Requirement already satisfied: scipy in c:\\users\\anami\\anaconda3\\lib\\site-packages (from pattern) (1.11.4)\n",
      "Requirement already satisfied: nltk in c:\\users\\anami\\anaconda3\\lib\\site-packages (from pattern) (3.8.1)\n",
      "Collecting python-docx (from pattern)\n",
      "  Downloading python_docx-1.1.2-py3-none-any.whl.metadata (2.0 kB)\n",
      "Collecting cherrypy (from pattern)\n",
      "  Downloading CherryPy-18.10.0-py3-none-any.whl.metadata (8.7 kB)\n",
      "Requirement already satisfied: requests in c:\\users\\anami\\anaconda3\\lib\\site-packages (from pattern) (2.31.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\anami\\anaconda3\\lib\\site-packages (from beautifulsoup4->pattern) (2.5)\n",
      "Collecting cheroot>=8.2.1 (from cherrypy->pattern)\n",
      "  Downloading cheroot-10.0.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting portend>=2.1.1 (from cherrypy->pattern)\n",
      "  Downloading portend-3.2.0-py3-none-any.whl.metadata (3.6 kB)\n",
      "Requirement already satisfied: more-itertools in c:\\users\\anami\\anaconda3\\lib\\site-packages (from cherrypy->pattern) (10.1.0)\n",
      "Collecting zc.lockfile (from cherrypy->pattern)\n",
      "  Downloading zc.lockfile-3.0.post1-py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting jaraco.collections (from cherrypy->pattern)\n",
      "  Downloading jaraco.collections-5.0.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting sgmllib3k (from feedparser->pattern)\n",
      "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: click in c:\\users\\anami\\anaconda3\\lib\\site-packages (from nltk->pattern) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\anami\\anaconda3\\lib\\site-packages (from nltk->pattern) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\anami\\anaconda3\\lib\\site-packages (from nltk->pattern) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\anami\\anaconda3\\lib\\site-packages (from nltk->pattern) (4.65.0)\n",
      "Requirement already satisfied: charset-normalizer>=2.0.0 in c:\\users\\anami\\anaconda3\\lib\\site-packages (from pdfminer.six->pattern) (2.0.4)\n",
      "Requirement already satisfied: cryptography>=36.0.0 in c:\\users\\anami\\anaconda3\\lib\\site-packages (from pdfminer.six->pattern) (42.0.2)\n",
      "Requirement already satisfied: typing-extensions>=4.9.0 in c:\\users\\anami\\anaconda3\\lib\\site-packages (from python-docx->pattern) (4.12.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\anami\\anaconda3\\lib\\site-packages (from requests->pattern) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\anami\\anaconda3\\lib\\site-packages (from requests->pattern) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\anami\\anaconda3\\lib\\site-packages (from requests->pattern) (2024.2.2)\n",
      "Collecting jaraco.functools (from cheroot>=8.2.1->cherrypy->pattern)\n",
      "  Downloading jaraco.functools-4.0.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: cffi>=1.12 in c:\\users\\anami\\anaconda3\\lib\\site-packages (from cryptography>=36.0.0->pdfminer.six->pattern) (1.16.0)\n",
      "Collecting tempora>=1.8 (from portend>=2.1.1->cherrypy->pattern)\n",
      "  Downloading tempora-5.6.0-py3-none-any.whl.metadata (3.1 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\anami\\anaconda3\\lib\\site-packages (from click->nltk->pattern) (0.4.6)\n",
      "Collecting jaraco.text (from jaraco.collections->cherrypy->pattern)\n",
      "  Downloading jaraco.text-3.12.1-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: setuptools in c:\\users\\anami\\anaconda3\\lib\\site-packages (from zc.lockfile->cherrypy->pattern) (68.2.2)\n",
      "Requirement already satisfied: pycparser in c:\\users\\anami\\anaconda3\\lib\\site-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six->pattern) (2.21)\n",
      "Collecting jaraco.context>=4.1 (from jaraco.text->jaraco.collections->cherrypy->pattern)\n",
      "  Downloading jaraco.context-5.3.0-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting autocommand (from jaraco.text->jaraco.collections->cherrypy->pattern)\n",
      "  Downloading autocommand-2.2.2-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: inflect in c:\\users\\anami\\anaconda3\\lib\\site-packages (from jaraco.text->jaraco.collections->cherrypy->pattern) (7.3.0)\n",
      "Collecting backports.tarfile (from jaraco.context>=4.1->jaraco.text->jaraco.collections->cherrypy->pattern)\n",
      "  Downloading backports.tarfile-1.2.0-py3-none-any.whl.metadata (2.0 kB)\n",
      "Requirement already satisfied: typeguard>=4.0.1 in c:\\users\\anami\\anaconda3\\lib\\site-packages (from inflect->jaraco.text->jaraco.collections->cherrypy->pattern) (4.3.0)\n",
      "Downloading backports.csv-1.0.7-py2.py3-none-any.whl (12 kB)\n",
      "Downloading CherryPy-18.10.0-py3-none-any.whl (349 kB)\n",
      "   ---------------------------------------- 0.0/349.8 kB ? eta -:--:--\n",
      "   --------------------------------------- 349.8/349.8 kB 22.6 MB/s eta 0:00:00\n",
      "Downloading feedparser-6.0.11-py3-none-any.whl (81 kB)\n",
      "   ---------------------------------------- 0.0/81.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 81.3/81.3 kB 4.7 MB/s eta 0:00:00\n",
      "Downloading mysqlclient-2.2.4-cp311-cp311-win_amd64.whl (203 kB)\n",
      "   ---------------------------------------- 0.0/203.2 kB ? eta -:--:--\n",
      "   --------------------------------------- 203.2/203.2 kB 12.9 MB/s eta 0:00:00\n",
      "Downloading pdfminer.six-20231228-py3-none-any.whl (5.6 MB)\n",
      "   ---------------------------------------- 0.0/5.6 MB ? eta -:--:--\n",
      "   -------------- ------------------------- 2.1/5.6 MB 43.8 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 3.7/5.6 MB 39.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  5.6/5.6 MB 39.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 5.6/5.6 MB 35.9 MB/s eta 0:00:00\n",
      "Downloading python_docx-1.1.2-py3-none-any.whl (244 kB)\n",
      "   ---------------------------------------- 0.0/244.3 kB ? eta -:--:--\n",
      "   --------------------------------------- 244.3/244.3 kB 14.6 MB/s eta 0:00:00\n",
      "Downloading cheroot-10.0.1-py3-none-any.whl (104 kB)\n",
      "   ---------------------------------------- 0.0/104.8 kB ? eta -:--:--\n",
      "   ---------------------------------------- 104.8/104.8 kB ? eta 0:00:00\n",
      "Downloading portend-3.2.0-py3-none-any.whl (5.3 kB)\n",
      "Downloading jaraco.collections-5.0.1-py3-none-any.whl (10 kB)\n",
      "Downloading zc.lockfile-3.0.post1-py3-none-any.whl (9.8 kB)\n",
      "Downloading tempora-5.6.0-py3-none-any.whl (13 kB)\n",
      "Downloading jaraco.functools-4.0.1-py3-none-any.whl (9.8 kB)\n",
      "Downloading jaraco.text-3.12.1-py3-none-any.whl (11 kB)\n",
      "Downloading jaraco.context-5.3.0-py3-none-any.whl (6.5 kB)\n",
      "Downloading autocommand-2.2.2-py3-none-any.whl (19 kB)\n",
      "Downloading backports.tarfile-1.2.0-py3-none-any.whl (30 kB)\n",
      "Building wheels for collected packages: pattern, sgmllib3k\n",
      "  Building wheel for pattern (setup.py): started\n",
      "  Building wheel for pattern (setup.py): finished with status 'done'\n",
      "  Created wheel for pattern: filename=Pattern-3.6-py3-none-any.whl size=22332713 sha256=e44bb54d037e0144761f2ae9ed48dbfba617d22313ca1a3953da5bf5fd95068f\n",
      "  Stored in directory: c:\\users\\anami\\appdata\\local\\pip\\cache\\wheels\\d0\\8d\\e1\\44b5ab952791ca07e66a780d11dbf141c49d151f0be21e811b\n",
      "  Building wheel for sgmllib3k (setup.py): started\n",
      "  Building wheel for sgmllib3k (setup.py): finished with status 'done'\n",
      "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6061 sha256=5c9632b7c646e790db74a9e79ce61e1a3e1dab18213cfde4cd4148343c6414ba\n",
      "  Stored in directory: c:\\users\\anami\\appdata\\local\\pip\\cache\\wheels\\3b\\25\\2a\\105d6a15df6914f4d15047691c6c28f9052cc1173e40285d03\n",
      "Successfully built pattern sgmllib3k\n",
      "Installing collected packages: sgmllib3k, backports.csv, zc.lockfile, python-docx, mysqlclient, jaraco.functools, feedparser, backports.tarfile, autocommand, tempora, jaraco.context, cheroot, portend, pdfminer.six, jaraco.text, jaraco.collections, cherrypy, pattern\n",
      "Successfully installed autocommand-2.2.2 backports.csv-1.0.7 backports.tarfile-1.2.0 cheroot-10.0.1 cherrypy-18.10.0 feedparser-6.0.11 jaraco.collections-5.0.1 jaraco.context-5.3.0 jaraco.functools-4.0.1 jaraco.text-3.12.1 mysqlclient-2.2.4 pattern-3.6 pdfminer.six-20231228 portend-3.2.0 python-docx-1.1.2 sgmllib3k-1.0.0 tempora-5.6.0 zc.lockfile-3.0.post1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# %pip install pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'popular'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     C:\\Users\\anami\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\cmudict.zip.\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     C:\\Users\\anami\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\gazetteers.zip.\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     C:\\Users\\anami\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\genesis.zip.\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     C:\\Users\\anami\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\gutenberg.zip.\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     C:\\Users\\anami\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\inaugural.zip.\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     C:\\Users\\anami\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\movie_reviews.zip.\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     C:\\Users\\anami\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\names.zip.\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     C:\\Users\\anami\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\shakespeare.zip.\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     C:\\Users\\anami\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     C:\\Users\\anami\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\treebank.zip.\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     C:\\Users\\anami\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\twitter_samples.zip.\n",
      "[nltk_data]    | Downloading package omw to\n",
      "[nltk_data]    |     C:\\Users\\anami\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package omw-1.4 to\n",
      "[nltk_data]    |     C:\\Users\\anami\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     C:\\Users\\anami\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet2021 to\n",
      "[nltk_data]    |     C:\\Users\\anami\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package wordnet31 to\n",
      "[nltk_data]    |     C:\\Users\\anami\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     C:\\Users\\anami\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     C:\\Users\\anami\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\words.zip.\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     C:\\Users\\anami\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping chunkers\\maxent_ne_chunker.zip.\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     C:\\Users\\anami\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     C:\\Users\\anami\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "children\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     C:\\Users\\anami\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection popular\n"
     ]
    }
   ],
   "source": [
    "# Python program to pluralize a given\n",
    "# word using pattern-en package\n",
    "\n",
    "# Import the NLTK module\n",
    "from pattern.en import pluralize\n",
    "from pattern.en import singularize\n",
    "import nltk\n",
    "\n",
    "# Installing NLTK data to import\n",
    "# and run en module of pattern\n",
    "nltk.download('popular')\n",
    "\n",
    "# Importing the pattern en module\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "children\n"
     ]
    }
   ],
   "source": [
    "# Define the word and make it plural\n",
    "# by using pluralize() function\n",
    "print(pluralize('child'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tomato', 'oysterss', 'potato', 'octopodes', 'fetas', 'eggss', 'oyster', 'octopu', 'tomatoess', 'egg', 'potatoes', 'fetum', 'fish'}\n"
     ]
    }
   ],
   "source": [
    "words = [\"potato\", \"tomatoes\", \"fish\", \"eggs\", \"octopus\", \"oysters\", \"feta\"]\n",
    "res1 = []\n",
    "for word in words:\n",
    "    # print(pluralize(word))\n",
    "    res1.append(pluralize(word))\n",
    "    res1.append(singularize(word))\n",
    "print(set(res1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Workflow for spell check and plurals\n",
    "\n",
    "1. User enters list of ingredients to include and exclude\n",
    "2. Call the spell checker, and correct the ingredients, print a message if an ingredient is not found. Can add a flag to check where the list is from and print to screen or not.\n",
    "3. From the corrected ingredients, singularize and pluralize everything, and make a set.\n",
    "4. Run this once more through the spell checker, to elimiate bad pluralization / singularization\n",
    "5. Now the list is ready to be vectorized, and sent to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "X has 2158 features, but NearestNeighbors is expecting 2170 features as input.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m yes_ing_series \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mSeries(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOkra\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m yes_ing_tx \u001b[38;5;241m=\u001b[39m vectorizer\u001b[38;5;241m.\u001b[39mtransform(yes_ing_series)\n\u001b[1;32m----> 5\u001b[0m distOfRes, indicesOfRes \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mkneighbors(yes_ing_tx)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# print the output\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m Result\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\anami\\anaconda3\\Lib\\site-packages\\sklearn\\neighbors\\_base.py:806\u001b[0m, in \u001b[0;36mKNeighborsMixin.kneighbors\u001b[1;34m(self, X, n_neighbors, return_distance)\u001b[0m\n\u001b[0;32m    804\u001b[0m         X \u001b[38;5;241m=\u001b[39m _check_precomputed(X)\n\u001b[0;32m    805\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 806\u001b[0m         X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_data(X, accept_sparse\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsr\u001b[39m\u001b[38;5;124m\"\u001b[39m, reset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, order\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    808\u001b[0m n_samples_fit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_samples_fit_\n\u001b[0;32m    809\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_neighbors \u001b[38;5;241m>\u001b[39m n_samples_fit:\n",
      "File \u001b[1;32mc:\\Users\\anami\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:588\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    585\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    587\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m--> 588\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_n_features(X, reset\u001b[38;5;241m=\u001b[39mreset)\n\u001b[0;32m    590\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[1;32mc:\\Users\\anami\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:389\u001b[0m, in \u001b[0;36mBaseEstimator._check_n_features\u001b[1;34m(self, X, reset)\u001b[0m\n\u001b[0;32m    386\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m    388\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_features \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_features_in_:\n\u001b[1;32m--> 389\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    390\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX has \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_features\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m features, but \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    391\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis expecting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_features_in_\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m features as input.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    392\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: X has 2158 features, but NearestNeighbors is expecting 2170 features as input."
     ]
    }
   ],
   "source": [
    "# Including ingredients only\n",
    "yes_ing_series = pd.Series(\"Okra\")\n",
    "yes_ing_tx = vectorizer.transform(yes_ing_series)\n",
    "\n",
    "distOfRes, indicesOfRes = model.kneighbors(yes_ing_tx)\n",
    "\n",
    "# print the output\n",
    "print(\"\\n Result\")\n",
    "\n",
    "for i in range(0, 11):  # TODO: 11 should be made configurable and match the n-neighbors number\n",
    "    name = df.loc[indicesOfRes[0][i], ['title']].values[0]\n",
    "    distance = (distOfRes[0][i]).round(3)\n",
    "    rating = df.loc[indicesOfRes[0][i], ['rating']].values[0]\n",
    "\n",
    "    print(f\"{name}  :  {distance}\")\n",
    "    # print(f\"{name}  :  {distance}  :  {rating}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Result\n",
      "Okra with Scallion, Lime, and Ginger  :  0.826\n",
      "Chicken, Sausage, and Okra Gumbo  :  0.83\n",
      "Broiled Tomato, Corn, and Okra  :  0.846\n",
      "Stewed Corn and Tomatoes with Okra  :  0.864\n",
      "Creole Chicken and Okra Gumbo  :  0.88\n",
      "Succotash  :  0.885\n",
      "Catfish and Okra with Pecan Butter Sauce  :  0.898\n",
      "Crisp Okra in Yogurt Sauce  :  0.898\n",
      "Chive Shortcakes with Smoky Corn and Okra Stew  :  0.902\n",
      "Corn and Okra Stew  :  0.905\n",
      "Spicy Gumbo-Laya  :  0.909\n"
     ]
    }
   ],
   "source": [
    "# Including and excluding ingredients\n",
    "yes_ing_series = pd.Series(\"Okra\")\n",
    "no_ing_series = pd.Series(\"Tomato Cilantro\")\n",
    "\n",
    "yes_ing_tx = vectorizer.transform(yes_ing_series)\n",
    "no_ing_tx = (vectorizer.transform(no_ing_series)) * -1\n",
    "\n",
    "updated_ing_tx = yes_ing_tx + no_ing_tx\n",
    "\n",
    "distOfRes, indicesOfRes = model.kneighbors(updated_ing_tx)\n",
    "\n",
    "# print the output\n",
    "print(\"\\n Result\")\n",
    "\n",
    "for i in range(0, 11):  # TODO: 11 should be made configurable and match the n-neighbors number\n",
    "    name = df.loc[indicesOfRes[0][i], ['title']].values[0]\n",
    "    distance = (distOfRes[0][i]).round(3)\n",
    "    rating = df.loc[indicesOfRes[0][i], ['rating']].values[0]\n",
    "\n",
    "    print(f\"{name}  :  {distance}\")\n",
    "    # print(f\"{name}  :  {distance}  :  {rating}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
